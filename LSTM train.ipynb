{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.constraints import MinMaxNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert .txt to .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations_txt/S8\"\n",
    "output_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations/S8\"\n",
    "\n",
    "# create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# iterate through files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            # construct input and output file paths\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            output_filepath = os.path.join(output_folder, os.path.splitext(filename)[0] + \".csv\")\n",
    "\n",
    "            # read txt file and remove leading and trailing quotation marks from each line\n",
    "            with open(input_filepath, 'r') as file:\n",
    "                lines = [line.strip().strip('\"') for line in file.readlines()]\n",
    "\n",
    "            # convert to df and save as csv\n",
    "            df = pd.DataFrame([line.split(',') for line in lines])\n",
    "            df.to_csv(output_filepath, index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "            print(f\"Converted {input_filepath} to {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {input_filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge data in batches to create the 'merged_data' folder containing all the available data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellcounts_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts'\n",
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "cellcounts_batch_size = 8\n",
    "cytokine_batch_size = 101\n",
    "\n",
    "for batch_index in range(1, cellcounts_batch_size + 1):\n",
    "    print(f\"Processing batch {batch_index}\")\n",
    "    cellcounts_file = os.path.join(cellcounts_folder, f'cellcount S{batch_index}.csv')\n",
    "    print(\"Cellcounts file:\", cellcounts_file)\n",
    "    cytokine_subfolder = f'S{batch_index}'\n",
    "    cytokine_folder = os.path.join(base_cytokine_folder, cytokine_subfolder)\n",
    "    df_cellcounts_batch = pd.read_csv(cellcounts_file)\n",
    "    print(\"Cellcounts batch shape:\", df_cellcounts_batch.shape)\n",
    "    df_cytokine_batch = pd.DataFrame()\n",
    "    for cytokine_file in os.listdir(cytokine_folder):\n",
    "        if cytokine_file.endswith('.csv'):\n",
    "            file_path = os.path.join(cytokine_folder, cytokine_file)\n",
    "            df_cytokine = pd.read_csv(file_path)\n",
    "            df_cytokine_batch = pd.concat([df_cytokine_batch, df_cytokine], ignore_index=True)\n",
    "\n",
    "    print(\"Cytokine batch shape:\", df_cytokine_batch.shape)\n",
    "\n",
    "    merged_data = pd.merge(df_cellcounts_batch, df_cytokine_batch, on='mcsteps')\n",
    "    print(\"Merged data shape:\", merged_data.shape)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f'combined_data_batch_{batch_index}.csv')\n",
    "    merged_data.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved merged data to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat the cytokines data to create the 'all_data' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/all_data'\n",
    "\n",
    "def create_simulation_subfolders(output_folder, simulation_names):\n",
    "    for sim_name in simulation_names:\n",
    "        sim_folder = os.path.join(output_folder, sim_name)\n",
    "        os.makedirs(sim_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def concat_files_in_folder(folder_path):\n",
    "    all_data = pd.DataFrame()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data = pd.concat([all_data, df])\n",
    "    return all_data\n",
    "\n",
    "def sort_by_mcsteps(data):\n",
    "    return data.sort_values(by='mcsteps')\n",
    "\n",
    "simulation_names = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8']\n",
    "create_simulation_subfolders(output_folder, simulation_names)\n",
    "\n",
    "for sim_name in simulation_names:\n",
    "    sim_folder_path = os.path.join(output_folder, sim_name)\n",
    "    sim_data = concat_files_in_folder(os.path.join(base_cytokine_folder, sim_name))\n",
    "    sorted_data = sort_by_mcsteps(sim_data)\n",
    "    output_file_path = os.path.join(sim_folder_path, f'{sim_name}_data_sorted.csv')\n",
    "    sorted_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data to cover all initializations (df1 to df8) and drop zCOM column as we have 2D spatial data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/all_data'\n",
    "\n",
    "def load_and_drop_zCOM(folder_path):\n",
    "    df = pd.concat([pd.read_csv(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.csv')])\n",
    "    df.drop(columns=['zCOM'], inplace=True)\n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "for i in range(1, 9):\n",
    "    sim_folder_path = os.path.join(output_folder, f'S{i}')\n",
    "    df = load_and_drop_zCOM(sim_folder_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df1, df2, df3, df4, df5, df6, df7, df8 = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print dataframe to make sure it works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of         mcsteps  xCOM  yCOM           il8           il1           il6  \\\n",
      "0             0   147   116  8.603181e-10  0.000000e+00  0.000000e+00   \n",
      "1             0   251   364  9.141505e-10  0.000000e+00  0.000000e+00   \n",
      "2             0   279   112  9.350631e-10  0.000000e+00  0.000000e+00   \n",
      "3             0    83   288  9.888261e-10  0.000000e+00  0.000000e+00   \n",
      "4             0   171   371  1.105985e-09  0.000000e+00  0.000000e+00   \n",
      "...         ...   ...   ...           ...           ...           ...   \n",
      "569270  1000000   172   192  1.493359e-08  1.195303e-11  3.136481e-27   \n",
      "569271  1000000   141   107  1.232809e-08  1.575511e-11  3.546226e-15   \n",
      "569272  1000000    81   257  1.258248e-08  1.729580e-11  1.002741e-11   \n",
      "569273  1000000   127   264  1.046952e-08  9.687031e-12  1.480615e-14   \n",
      "569274  1000000   298   363  1.142387e-08  7.674055e-12  5.401783e-25   \n",
      "\n",
      "                il10           tnf           tgf  \n",
      "0       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...              ...           ...           ...  \n",
      "569270  0.000000e+00  5.448368e-14  3.486646e-11  \n",
      "569271  4.554151e-23  9.951106e-22  2.055369e-10  \n",
      "569272  1.976363e-14  5.256890e-17  9.035102e-11  \n",
      "569273  1.299817e-22  2.521331e-17  8.664754e-11  \n",
      "569274  1.290000e-43  7.946154e-12  9.634385e-12  \n",
      "\n",
      "[569275 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df8.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an array for each unique mcsteps value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "# Get unique 'mcsteps' values\n",
    "unique_mcsteps = df8['mcsteps'].unique()\n",
    "\n",
    "# Define array to store results\n",
    "arrays = {}\n",
    "\n",
    "# Iterate over unique 'mcsteps' values\n",
    "for mcstep in unique_mcsteps:\n",
    "    # Filter data for current 'mcsteps' value\n",
    "    df_mcstep = df8[df8['mcsteps'] == mcstep]\n",
    "    \n",
    "    # Initialize 500x500 array for current 'mcsteps' value\n",
    "    array = np.zeros((500, 500, len(cytokines)))\n",
    "    \n",
    "    # Iterate over rows in filtered DataFrame\n",
    "    for index, row in df_mcstep.iterrows():\n",
    "        # Get X and Y coordinates\n",
    "        x = int(row['xCOM'])\n",
    "        y = int(row['yCOM'])\n",
    "        \n",
    "        # Get cytokine concentrations\n",
    "        concentrations = row[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "        \n",
    "        # Assign cytokine concentrations to corresponding position in array\n",
    "        array[x, y] = concentrations\n",
    "    \n",
    "    # Store array for current 'mcsteps' value\n",
    "    arrays[mcstep] = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print to make sure array works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays: 101\n",
      "Shape of the array: (500, 500, 6)\n",
      "Value at position (356,200): [1.1378244e-08 2.0458439e-11 5.3863960e-15 5.2224680e-23 7.6039260e-18\n",
      " 1.2120631e-11]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of arrays:\", len(arrays))\n",
    "array = arrays[1000000]\n",
    "print(\"Shape of the array:\", array.shape)\n",
    "print(\"Value at position (356,200):\", array[356,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "65",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(arrays), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Placeholder for target data, adjust as per your actual targets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Split data into train and test sets\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m arrays_train, arrays_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Stack the arrays to create a 4D array\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(arrays_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_split.py:2672\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_split.py:2674\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2674\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2675\u001b[0m     )\n\u001b[0;32m   2676\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\__init__.py:357\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_list_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\__init__.py:211\u001b[0m, in \u001b[0;36m_list_indexing\u001b[1;34m(X, key, key_dtype)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [X[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\__init__.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compress(X, key))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# key is a integer array-like of key\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 65"
     ]
    }
   ],
   "source": [
    "# Assuming you have an array named arrays where each element represents an array with shape (500, 500, 6)\n",
    "# The indices for accessing the arrays are distinct and range from some arbitrary values\n",
    "\n",
    "# Generate corresponding target data (just as placeholders)\n",
    "y = np.random.rand(len(arrays), 2)  # Placeholder for target data, adjust as per your actual targets\n",
    "\n",
    "# Split data into train and test sets\n",
    "arrays_train, arrays_test, y_train, y_test = train_test_split(arrays, y, test_size=0.1, random_state=30000)\n",
    "\n",
    "# Stack the arrays to create a 4D array\n",
    "X_train = np.stack(arrays_train)\n",
    "X_test = np.stack(arrays_test)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(500, 6), return_sequences=True),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    LSTM(256),\n",
    "    Dense(2)  # Assuming 2 output values\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "print('Predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    " class Model(tf.keras.Model):\n",
    "    def __init__(self, model_path, train_mode=True, input_dim=19, lstm_size=500, batch_size=4, e_learning_rate=1e-5):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.train_mode = train_mode\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.e_learning_rate = e_learning_rate\n",
    "\n",
    "        #define LSTM layer\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(units=self.lstm_size, return_sequences=True)\n",
    "\n",
    "        #define output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, activation=None)\n",
    "        # Define reshape layer\n",
    "        self.reshape_layer = tf.keras.layers.Reshape((500, 500))  # Reshape to match spatial dimensions\n",
    "\n",
    "        #define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.e_learning_rate)\n",
    "\n",
    "        #define MSE as loss function\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Reshape inputs to add sequence length dimension\n",
    "        inputs = tf.expand_dims(inputs, axis=1)\n",
    "        \n",
    "        # Input shape: (batch_size, sequence_length, input_dim)\n",
    "        x = self.lstm_layer(inputs)\n",
    "        # Output shape: (batch_size, sequence_length, lstm_size)\n",
    "        output = self.output_layer(x)\n",
    "        # Output shape: (batch_size, sequence_length, 2) - 2 for outputs\n",
    "        \n",
    "        # Reshape output to match spatial dimensions\n",
    "        output = self.reshape_layer(output)\n",
    "        return output\n",
    "\n",
    "    def train_step(self, xtrain, ytrain):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(xtrain, training=True)\n",
    "            loss = self.loss_fn(ytrain, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_set, valid_set, maxEpoch=10):\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        \n",
    "        for epoch in range(maxEpoch):\n",
    "            train_loss = self.train_step(x_train, y_train)\n",
    "            valid_loss = self.loss_fn(y_valid, self(x_valid, training=False))\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df1 = data_frames[0]\n",
    "    \n",
    "    #convert the df to numpy array and cast the data to float\n",
    "    results = df1.to_numpy(dtype='float')\n",
    "\n",
    "    #define input indices and output indices\n",
    "    input_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "    output_indices = [11, 12] \n",
    "\n",
    "   # Split data into train and valid sets\n",
    "    train_size = int(len(results) * 0.9)\n",
    "    train_features = results[:train_size, input_indices]\n",
    "    train_targets = results[:train_size, output_indices]\n",
    "    valid_features = results[train_size:, input_indices]\n",
    "    valid_targets = results[train_size:, output_indices]\n",
    "\n",
    "    # Create train and valid sets with input features and targets\n",
    "    train_set = (train_features, train_targets)\n",
    "    valid_set = (valid_features, valid_targets)\n",
    "\n",
    "    #initialize and train the model\n",
    "    mymodel = Model(model_path=\"saved_model\")\n",
    "    mymodel.train(train_set, valid_set, maxEpoch=500)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of        mcsteps     1       2      3      4     5    6      7    8      9  \\\n",
      "0            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "1            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "2            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "3            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "4            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "...        ...   ...     ...    ...    ...   ...  ...    ...  ...    ...   \n",
      "74753  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74754  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74755  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74756  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74757  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "\n",
      "          10  xCOM  yCOM           il8           il1           il6  \\\n",
      "0       29.0    84   376  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "1       29.0   432   181  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "2       29.0   409   105  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "3       29.0   247   394  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "4       29.0   132   141  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "...      ...   ...   ...           ...           ...           ...   \n",
      "74753  163.0   468   284 -5.413690e-13  2.505693e-11  3.018151e-10   \n",
      "74754  163.0   475   433 -1.276491e-12  7.168744e-11  1.215652e-11   \n",
      "74755  163.0   425     3 -6.611300e-13  4.721322e-11  6.383543e-12   \n",
      "74756  163.0     2   475 -1.999419e-12  1.093847e-10  6.594112e-12   \n",
      "74757  163.0   262   493 -6.534840e-13  3.342928e-11  7.318865e-12   \n",
      "\n",
      "               il10           tnf           tgf  \n",
      "0      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...             ...           ...           ...  \n",
      "74753  7.795065e-11  7.162435e-11  5.866973e-10  \n",
      "74754  1.033143e-14  2.937644e-13  7.248822e-10  \n",
      "74755  8.737277e-16  4.176781e-10  4.689090e-10  \n",
      "74756  1.326679e-15  6.911885e-14  6.668235e-10  \n",
      "74757  2.317959e-15  5.880362e-14  4.089780e-10  \n",
      "\n",
      "[74758 rows x 19 columns]>\n",
      "(74758, 19)\n"
     ]
    }
   ],
   "source": [
    "print(df1.info)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = data_frames[6]\n",
    "# separate features (X) and target variables (y)\n",
    "X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "y = df7[['xCOM', 'yCOM']].values\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# reshape input data to 3D for LSTM input\n",
    "# the input shape is samples, timesteps, features\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.001) # learning rate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))  # dropout regularization\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(2))  # output layer with 2 neurons for xCOM and yCOM\n",
    "model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "\n",
    "# train\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=128, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "# evaluate\n",
    "loss = model.evaluate(X_test_reshaped, y_test)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "predictions = model.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1652/1652 [==============================] - 35s 19ms/step - loss: 30146.3242 - val_loss: 15526.3955\n",
      "Epoch 2/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15247.3193 - val_loss: 15029.4688\n",
      "Epoch 3/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15185.9131 - val_loss: 15030.0566\n",
      "Epoch 4/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15185.9229 - val_loss: 15029.5264\n",
      "Epoch 5/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.8623 - val_loss: 15032.1777\n",
      "Epoch 6/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.4238 - val_loss: 15029.5645\n",
      "Epoch 7/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.5322 - val_loss: 15030.3740\n",
      "Epoch 8/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.2100 - val_loss: 15030.1895\n",
      "Epoch 9/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.0947 - val_loss: 15030.2539\n",
      "Epoch 10/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15186.3643 - val_loss: 15029.8496\n",
      "Epoch 11/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.1641 - val_loss: 15030.0615\n",
      "Epoch 12/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.2090 - val_loss: 15030.4922\n",
      "Epoch 13/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15182.5498 - val_loss: 14987.3828\n",
      "Epoch 14/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15113.1279 - val_loss: 14935.3975\n",
      "Epoch 15/50\n",
      "1652/1652 [==============================] - 32s 19ms/step - loss: 15067.1807 - val_loss: 14901.7793\n",
      "Epoch 16/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15041.3330 - val_loss: 14876.9375\n",
      "Epoch 17/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15012.5322 - val_loss: 14842.7705\n",
      "Epoch 18/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14972.0625 - val_loss: 14795.4277\n",
      "Epoch 19/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14913.0234 - val_loss: 14731.7500\n",
      "Epoch 20/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14837.8486 - val_loss: 14634.2949\n",
      "Epoch 21/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14755.2070 - val_loss: 14575.2725\n",
      "Epoch 22/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14656.5449 - val_loss: 14467.7188\n",
      "Epoch 23/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14548.2734 - val_loss: 14363.7637\n",
      "Epoch 24/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14430.5869 - val_loss: 14245.7949\n",
      "Epoch 25/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14313.6738 - val_loss: 14145.7783\n",
      "Epoch 26/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14189.7178 - val_loss: 14020.6602\n",
      "Epoch 27/50\n",
      "1652/1652 [==============================] - 32s 19ms/step - loss: 14069.5820 - val_loss: 13924.1328\n",
      "Epoch 28/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13949.5957 - val_loss: 13827.8369\n",
      "Epoch 29/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13835.5068 - val_loss: 13719.3887\n",
      "Epoch 30/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13727.9609 - val_loss: 13661.8389\n",
      "Epoch 31/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13621.7939 - val_loss: 13553.1826\n",
      "Epoch 32/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13517.4229 - val_loss: 13445.6133\n",
      "Epoch 33/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13419.6719 - val_loss: 13353.1934\n",
      "Epoch 34/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 13324.2412 - val_loss: 13286.1846\n",
      "Epoch 35/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13228.3594 - val_loss: 13214.7236\n",
      "Epoch 36/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13142.9346 - val_loss: 13153.7041\n",
      "Epoch 37/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 13050.9316 - val_loss: 13041.5938\n",
      "Epoch 38/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12972.0928 - val_loss: 12988.6113\n",
      "Epoch 39/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12886.8721 - val_loss: 12911.6611\n",
      "Epoch 40/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12809.9844 - val_loss: 12877.7666\n",
      "Epoch 41/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12739.7412 - val_loss: 12804.1787\n",
      "Epoch 42/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12663.8633 - val_loss: 12785.6348\n",
      "Epoch 43/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12595.9541 - val_loss: 12691.6367\n",
      "Epoch 44/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12527.5186 - val_loss: 12634.2559\n",
      "Epoch 45/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12458.5781 - val_loss: 12609.5020\n",
      "Epoch 46/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12393.8613 - val_loss: 12573.6035\n",
      "Epoch 47/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12335.5117 - val_loss: 12545.0205\n",
      "Epoch 48/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12273.2949 - val_loss: 12480.3174\n",
      "Epoch 49/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12213.8906 - val_loss: 12414.9141\n",
      "Epoch 50/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12154.4375 - val_loss: 12387.5439\n",
      "1468/1468 [==============================] - 3s 2ms/step - loss: 12387.5430\n",
      "Test Loss: 12387.54296875\n",
      "1468/1468 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "class MyLSTMModel:\n",
    "    def __init__(self, data_frames):\n",
    "        self.data_frames = data_frames\n",
    "        self.X_train_reshaped = None\n",
    "        self.X_test_reshaped = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self, test_size=0.1):\n",
    "        df7 = self.data_frames[6]\n",
    "        X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "        y = df7[['xCOM', 'yCOM']].values\n",
    "\n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        self.X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "    def build_model(self, optimizer='adam'):\n",
    "        custom_optimizer = Adam(learning_rate=0.001) if optimizer == 'adam' else optimizer\n",
    "\n",
    "        self.model = Sequential([\n",
    "            LSTM(256, input_shape=(self.X_train_reshaped.shape[1], self.X_train_reshaped.shape[2]), return_sequences=True),\n",
    "            LSTM(256, return_sequences=True),\n",
    "            LSTM(256),\n",
    "            Dense(2)\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "\n",
    "    def train_model(self, epochs=50, batch_size=256):\n",
    "        self.model.fit(self.X_train_reshaped, self.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.X_test_reshaped, self.y_test))\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        loss = self.model.evaluate(self.X_test_reshaped, self.y_test)\n",
    "        print('Test Loss:', loss)\n",
    "\n",
    "    def predict(self):\n",
    "        predictions = self.model.predict(self.X_test_reshaped)\n",
    "        return predictions\n",
    "\n",
    "data_frames\n",
    "lstm_model = MyLSTMModel(data_frames)\n",
    "lstm_model.prepare_data()\n",
    "lstm_model.build_model()\n",
    "lstm_model.train_model()\n",
    "lstm_model.evaluate_model()\n",
    "predictions = lstm_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
