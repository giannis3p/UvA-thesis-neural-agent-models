{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.constraints import MinMaxNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert .txt to .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations_txt/S8\"\n",
    "output_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations/S8\"\n",
    "\n",
    "# create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# iterate through files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            # construct input and output file paths\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            output_filepath = os.path.join(output_folder, os.path.splitext(filename)[0] + \".csv\")\n",
    "\n",
    "            # read txt file and remove leading and trailing quotation marks from each line\n",
    "            with open(input_filepath, 'r') as file:\n",
    "                lines = [line.strip().strip('\"') for line in file.readlines()]\n",
    "\n",
    "            # convert to df and save as csv\n",
    "            df = pd.DataFrame([line.split(',') for line in lines])\n",
    "            df.to_csv(output_filepath, index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "            print(f\"Converted {input_filepath} to {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {input_filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge data in batches to create the 'merged_data' folder containing all the available data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellcounts_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts'\n",
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "cellcounts_batch_size = 8\n",
    "cytokine_batch_size = 101\n",
    "\n",
    "for batch_index in range(1, cellcounts_batch_size + 1):\n",
    "    print(f\"Processing batch {batch_index}\")\n",
    "    cellcounts_file = os.path.join(cellcounts_folder, f'cellcount S{batch_index}.csv')\n",
    "    print(\"Cellcounts file:\", cellcounts_file)\n",
    "    cytokine_subfolder = f'S{batch_index}'\n",
    "    cytokine_folder = os.path.join(base_cytokine_folder, cytokine_subfolder)\n",
    "    df_cellcounts_batch = pd.read_csv(cellcounts_file)\n",
    "    print(\"Cellcounts batch shape:\", df_cellcounts_batch.shape)\n",
    "    df_cytokine_batch = pd.DataFrame()\n",
    "    for cytokine_file in os.listdir(cytokine_folder):\n",
    "        if cytokine_file.endswith('.csv'):\n",
    "            file_path = os.path.join(cytokine_folder, cytokine_file)\n",
    "            df_cytokine = pd.read_csv(file_path)\n",
    "            df_cytokine_batch = pd.concat([df_cytokine_batch, df_cytokine], ignore_index=True)\n",
    "\n",
    "    print(\"Cytokine batch shape:\", df_cytokine_batch.shape)\n",
    "\n",
    "    merged_data = pd.merge(df_cellcounts_batch, df_cytokine_batch, on='mcsteps')\n",
    "    print(\"Merged data shape:\", merged_data.shape)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f'combined_data_batch_{batch_index}.csv')\n",
    "    merged_data.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved merged data to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat the cytokines data to create the 'all_data' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/all_data'\n",
    "\n",
    "def create_simulation_subfolders(output_folder, simulation_names):\n",
    "    for sim_name in simulation_names:\n",
    "        sim_folder = os.path.join(output_folder, sim_name)\n",
    "        os.makedirs(sim_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def concat_files_in_folder(folder_path):\n",
    "    all_data = pd.DataFrame()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data = pd.concat([all_data, df])\n",
    "    return all_data\n",
    "\n",
    "def sort_by_mcsteps(data):\n",
    "    return data.sort_values(by='mcsteps')\n",
    "\n",
    "simulation_names = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8']\n",
    "create_simulation_subfolders(output_folder, simulation_names)\n",
    "\n",
    "for sim_name in simulation_names:\n",
    "    sim_folder_path = os.path.join(output_folder, sim_name)\n",
    "    sim_data = concat_files_in_folder(os.path.join(base_cytokine_folder, sim_name))\n",
    "    sorted_data = sort_by_mcsteps(sim_data)\n",
    "    output_file_path = os.path.join(sim_folder_path, f'{sim_name}_data_sorted.csv')\n",
    "    sorted_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data to cover all initializations (df1 to df8) and drop zCOM column as we have 2D spatial data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"C:/Users/Giannis/Documents/uva-thesis/data/all_data\"\n",
    "\n",
    "def load_and_drop_zCOM(folder_path):\n",
    "    df = pd.concat([pd.read_csv(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.csv')])\n",
    "    df.drop(columns=['zCOM'], inplace=True)\n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "for i in range(1, 9):\n",
    "    sim_folder_path = os.path.join(output_folder, f'S{i}')\n",
    "    df = load_and_drop_zCOM(sim_folder_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df1, df2, df3, df4, df5, df6, df7, df8 = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change 'mcsteps' to 'time' and print dataframe to make sure it works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time  xCOM  yCOM           il8           il1           il6  \\\n",
      "0          0   147   116  8.603181e-10  0.000000e+00  0.000000e+00   \n",
      "1          0   251   364  9.141505e-10  0.000000e+00  0.000000e+00   \n",
      "2          0   279   112  9.350631e-10  0.000000e+00  0.000000e+00   \n",
      "3          0    83   288  9.888261e-10  0.000000e+00  0.000000e+00   \n",
      "4          0   171   371  1.105985e-09  0.000000e+00  0.000000e+00   \n",
      "...      ...   ...   ...           ...           ...           ...   \n",
      "569270   100   172   192  1.493359e-08  1.195303e-11  3.136481e-27   \n",
      "569271   100   141   107  1.232809e-08  1.575511e-11  3.546226e-15   \n",
      "569272   100    81   257  1.258248e-08  1.729580e-11  1.002741e-11   \n",
      "569273   100   127   264  1.046952e-08  9.687031e-12  1.480615e-14   \n",
      "569274   100   298   363  1.142387e-08  7.674055e-12  5.401783e-25   \n",
      "\n",
      "                il10           tnf           tgf  \n",
      "0       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...              ...           ...           ...  \n",
      "569270  0.000000e+00  5.448368e-14  3.486646e-11  \n",
      "569271  4.554151e-23  9.951106e-22  2.055369e-10  \n",
      "569272  1.976363e-14  5.256890e-17  9.035102e-11  \n",
      "569273  1.299817e-22  2.521331e-17  8.664754e-11  \n",
      "569274  1.290000e-43  7.946154e-12  9.634385e-12  \n",
      "\n",
      "[569275 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giannis\\AppData\\Local\\Temp\\ipykernel_12340\\2445232496.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df8.drop(columns=['mcsteps'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df8['time'] = (df8['mcsteps'] / 10000).astype(int)\n",
    "df8 = df8[['time'] + [col for col in df8.columns if col != 'time']]\n",
    "df8.drop(columns=['mcsteps'], inplace=True)\n",
    "print(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest values for each cytokine:\n",
      "il8    -1.802096e-11\n",
      "il1     0.000000e+00\n",
      "il6     0.000000e+00\n",
      "il10    0.000000e+00\n",
      "tnf     0.000000e+00\n",
      "tgf     0.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "Largest values for each cytokine:\n",
      "il8     1.817857e-08\n",
      "il1     7.833237e-09\n",
      "il6     6.361151e-09\n",
      "il10    1.409022e-09\n",
      "tnf     5.365540e-08\n",
      "tgf     1.111927e-08\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select columns containing cytokine data\n",
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# Find the smallest values for each cytokine\n",
    "smallest_values = df8[cytokine_columns].min()\n",
    "\n",
    "# Find the largest values for each cytokine\n",
    "largest_values = df8[cytokine_columns].max()\n",
    "\n",
    "print(\"Smallest values for each cytokine:\")\n",
    "print(smallest_values)\n",
    "\n",
    "print(\"\\nLargest values for each cytokine:\")\n",
    "print(largest_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an array for each unique mcsteps value, should taken a couple minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# get unique time values\n",
    "unique_time = df8['time'].unique()\n",
    "\n",
    "arrays = {}\n",
    "\n",
    "# iterate over unique time values\n",
    "for time in unique_time:\n",
    "    # filter data for current value of time\n",
    "    df_time = df8[df8['time'] == time]\n",
    "    \n",
    "    # initialize 500x500 array for current value of time\n",
    "    array = np.zeros((500, 500, len(cytokines)))\n",
    "    \n",
    "    # iterate over rows in filtered df\n",
    "    for index, row in df_time.iterrows():\n",
    "        # get X and Y coordinates\n",
    "        x = int(row['xCOM'])\n",
    "        y = int(row['yCOM'])\n",
    "        \n",
    "        # get cytokine concentrations\n",
    "        concentrations = row[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "        \n",
    "        # assign cytokine concentrations to corresponding position in array\n",
    "        array[x, y] = concentrations\n",
    "    \n",
    "    # store array for current value of time\n",
    "    arrays[time] = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print to make sure array works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays: 101\n",
      "Shape of the array: (500, 500, 6)\n",
      "Value at position (356,200): [1.1378244e-08 2.0458439e-11 5.3863960e-15 5.2224680e-23 7.6039260e-18\n",
      " 1.2120631e-11]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of arrays:\", len(arrays))\n",
    "array = arrays[100]\n",
    "print(\"Shape of the array:\", array.shape)\n",
    "print(\"Value at position (356,200):\", array[356,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create input_sequences and output_values for the LSTM to use in order to be able to predict output_values from input_sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "output_values = []\n",
    "\n",
    "# convert dictionary values to a list of arrays\n",
    "arrays_list = [arrays[key] for key in sorted(arrays.keys())]\n",
    "\n",
    "# convert 'arrays' list to numpy array\n",
    "arrays_np = np.array(arrays_list)\n",
    "\n",
    "for i in range(len(arrays_np) - sequence_length):\n",
    "    input_seq = arrays_np[i:i+sequence_length]  # input sequence of arrays\n",
    "    output_val = arrays_np[i+sequence_length]   # array at next time step\n",
    "    \n",
    "    input_sequences.append(input_seq)\n",
    "    output_values.append(output_val)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_values = np.array(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_sequences has a shape of (91, 10, 500, 500, 6), which means we have 91 samples, each consisting of 10 arrays of shape (500, 500, 6).**\n",
    "\n",
    "**output_values has a shape of (91, 500, 500, 6), indicating that each sample has an output array of shape (500, 500, 6).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 2, 500, 500, 6)\n",
      "(99, 500, 500, 6)\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences.shape)\n",
    "print(output_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 10 for a sequence length of 10 as defined above\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model.add(Dense(units=100, activation='relu'))  # 100 neurons, first hidden layer, relu\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#model.add(Dense(units=100, activation='relu'))  # 100 neurons, second hidden layer, relu\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# output layer, linear activation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(10, 500 * 500 * 6)))  # 10 for a sequence length of 10 as defined above\n",
    "#model.add(Dense(units=100, activation='relu'))  # 100 neurons, first hidden layer, relu\n",
    "#model.add(Dense(units=100, activation='relu'))  # 100 neurons, second hidden layer, relu\n",
    "model.add(Dense(units=500 * 500 * 6, activation='linear'))  # output layer, linear activation\n",
    "model.add(Reshape((500, 500, 6)))\n",
    "model.compile(optimizer='adam', loss='mse')  # compile with adam, mse\n",
    "print(model.summary())\n",
    "\n",
    "input_sequences_reshaped = input_sequences.reshape(input_sequences.shape[0], 10, -1)\n",
    "\n",
    "# train\n",
    "history = model.fit(input_sequences_reshaped, output_values, epochs=10, batch_size=32, validation_split=0.2)\n",
    "print(\"Training Loss:\", history.history['loss'])\n",
    "\n",
    "# evaluate\n",
    "loss = model.evaluate(input_sequences, output_values)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stuff I tried previously**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, model_path, train_mode=True, input_dim=19, lstm_size=500, batch_size=4, e_learning_rate=1e-5):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.train_mode = train_mode\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.e_learning_rate = e_learning_rate\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(units=self.lstm_size, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, activation=None)\n",
    "        self.reshape_layer = tf.keras.layers.Reshape((500, 500))\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.e_learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.expand_dims(inputs, axis=1)\n",
    "        x = self.lstm_layer(inputs)\n",
    "        output = self.output_layer(x)\n",
    "        output = self.reshape_layer(output)\n",
    "        return output\n",
    "\n",
    "    def train_step(self, xtrain, ytrain):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(xtrain, training=True)\n",
    "            loss = self.loss_fn(ytrain, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_set, valid_set, maxEpoch=10):\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        \n",
    "        for epoch in range(maxEpoch):\n",
    "            train_loss = self.train_step(x_train, y_train)\n",
    "            valid_loss = self.loss_fn(y_valid, self(x_valid, training=False))\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df1 = data_frames[0]\n",
    "    \n",
    "    #convert the df to numpy array and cast the data to float\n",
    "    results = df1.to_numpy(dtype='float')\n",
    "\n",
    "    #define input indices and output indices\n",
    "    input_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "    output_indices = [11, 12] \n",
    "\n",
    "   # Split data into train and valid sets\n",
    "    train_size = int(len(results) * 0.9)\n",
    "    train_features = results[:train_size, input_indices]\n",
    "    train_targets = results[:train_size, output_indices]\n",
    "    valid_features = results[train_size:, input_indices]\n",
    "    valid_targets = results[train_size:, output_indices]\n",
    "\n",
    "    # Create train and valid sets with input features and targets\n",
    "    train_set = (train_features, train_targets)\n",
    "    valid_set = (valid_features, valid_targets)\n",
    "\n",
    "    #initialize and train the model\n",
    "    mymodel = Model(model_path=\"saved_model\")\n",
    "    mymodel.train(train_set, valid_set, maxEpoch=500)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = data_frames[6]\n",
    "X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "y = df7[['xCOM', 'yCOM']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(2))  # output layer with 2 neurons for xCOM and yCOM\n",
    "model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "# train\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=128, validation_data=(X_test_reshaped, y_test))\n",
    "# evaluate\n",
    "loss = model.evaluate(X_test_reshaped, y_test)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "predictions = model.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMModel:\n",
    "    def __init__(self, data_frames):\n",
    "        self.data_frames = data_frames\n",
    "        self.X_train_reshaped = None\n",
    "        self.X_test_reshaped = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self, test_size=0.1):\n",
    "        df7 = self.data_frames[6]\n",
    "        X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "        y = df7[['xCOM', 'yCOM']].values\n",
    "\n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        self.X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "    def build_model(self, optimizer='adam'):\n",
    "        custom_optimizer = Adam(learning_rate=0.001) if optimizer == 'adam' else optimizer\n",
    "\n",
    "        self.model = Sequential([\n",
    "            LSTM(256, input_shape=(self.X_train_reshaped.shape[1], self.X_train_reshaped.shape[2]), return_sequences=True),\n",
    "            LSTM(256, return_sequences=True),\n",
    "            LSTM(256),\n",
    "            Dense(2)\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "\n",
    "    def train_model(self, epochs=50, batch_size=256):\n",
    "        self.model.fit(self.X_train_reshaped, self.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.X_test_reshaped, self.y_test))\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        loss = self.model.evaluate(self.X_test_reshaped, self.y_test)\n",
    "        print('Test Loss:', loss)\n",
    "\n",
    "    def predict(self):\n",
    "        predictions = self.model.predict(self.X_test_reshaped)\n",
    "        return predictions\n",
    "\n",
    "data_frames\n",
    "lstm_model = MyLSTMModel(data_frames)\n",
    "lstm_model.prepare_data()\n",
    "lstm_model.build_model()\n",
    "lstm_model.train_model()\n",
    "lstm_model.evaluate_model()\n",
    "predictions = lstm_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
