{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.constraints import MinMaxNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert .txt to .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations_txt/S8\"\n",
    "output_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations/S8\"\n",
    "\n",
    "# create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# iterate through files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            # construct input and output file paths\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            output_filepath = os.path.join(output_folder, os.path.splitext(filename)[0] + \".csv\")\n",
    "\n",
    "            # read txt file and remove leading and trailing quotation marks from each line\n",
    "            with open(input_filepath, 'r') as file:\n",
    "                lines = [line.strip().strip('\"') for line in file.readlines()]\n",
    "\n",
    "            # convert to df and save as csv\n",
    "            df = pd.DataFrame([line.split(',') for line in lines])\n",
    "            df.to_csv(output_filepath, index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "            print(f\"Converted {input_filepath} to {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {input_filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge data in batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S1.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (74758, 10)\n",
      "Merged data shape: (74758, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_1.csv\n",
      "Processing batch 2\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S2.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (83259, 10)\n",
      "Merged data shape: (83259, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_2.csv\n",
      "Processing batch 3\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S3.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (123611, 10)\n",
      "Merged data shape: (123611, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_3.csv\n",
      "Processing batch 4\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S4.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (174195, 10)\n",
      "Merged data shape: (174195, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_4.csv\n",
      "Processing batch 5\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S5.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (274522, 10)\n",
      "Merged data shape: (274522, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_5.csv\n",
      "Processing batch 6\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S6.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (371380, 10)\n",
      "Merged data shape: (371380, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_6.csv\n",
      "Processing batch 7\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S7.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (469718, 10)\n",
      "Merged data shape: (469718, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_7.csv\n",
      "Processing batch 8\n",
      "Cellcounts file: C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts\\cellcount S8.csv\n",
      "Cellcounts batch shape: (101, 11)\n",
      "Cytokine batch shape: (569275, 10)\n",
      "Merged data shape: (569275, 20)\n",
      "Saved merged data to C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data\\combined_data_batch_8.csv\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "cellcounts_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts'\n",
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Define batch size\n",
    "cellcounts_batch_size = 8\n",
    "cytokine_batch_size = 101\n",
    "\n",
    "# Iterate over batches\n",
    "for batch_index in range(1, cellcounts_batch_size + 1):\n",
    "    print(f\"Processing batch {batch_index}\")\n",
    "\n",
    "    # Get cellcounts file for the current batch\n",
    "    cellcounts_file = os.path.join(cellcounts_folder, f'cellcount S{batch_index}.csv')\n",
    "    print(\"Cellcounts file:\", cellcounts_file)\n",
    "\n",
    "    # Get cytokine subfolder for the current batch\n",
    "    cytokine_subfolder = f'S{batch_index}'\n",
    "    cytokine_folder = os.path.join(base_cytokine_folder, cytokine_subfolder)\n",
    "\n",
    "    # Read cellcounts data\n",
    "    df_cellcounts_batch = pd.read_csv(cellcounts_file)\n",
    "    print(\"Cellcounts batch shape:\", df_cellcounts_batch.shape)\n",
    "\n",
    "    # Initialize an empty DataFrame to store cytokine data\n",
    "    df_cytokine_batch = pd.DataFrame()\n",
    "\n",
    "    # Iterate over cytokine files in the subfolder\n",
    "    for cytokine_file in os.listdir(cytokine_folder):\n",
    "        if cytokine_file.endswith('.csv'):\n",
    "            file_path = os.path.join(cytokine_folder, cytokine_file)\n",
    "            df_cytokine = pd.read_csv(file_path)\n",
    "            df_cytokine_batch = pd.concat([df_cytokine_batch, df_cytokine], ignore_index=True)\n",
    "\n",
    "    print(\"Cytokine batch shape:\", df_cytokine_batch.shape)\n",
    "\n",
    "    # Merge cellcounts and cytokine data\n",
    "    merged_data = pd.merge(df_cellcounts_batch, df_cytokine_batch, on='mcsteps')\n",
    "    print(\"Merged data shape:\", merged_data.shape)\n",
    "\n",
    "    # Save merged data to a new CSV file\n",
    "    output_filename = os.path.join(output_folder, f'combined_data_batch_{batch_index}.csv')\n",
    "    merged_data.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved merged data to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data to cover all initializations and drop zCOM column as we have 2D spatial data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74758, 19)\n",
      "   mcsteps     1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0  0.0  29.0   \n",
      "1        0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0  0.0  29.0   \n",
      "2        0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0  0.0  29.0   \n",
      "3        0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0  0.0  29.0   \n",
      "4        0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0  0.0  29.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0    84   376  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1   432   181  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   409   105  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   247   394  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   132   141  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(83259, 19)\n",
      "   mcsteps      1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  100.0  1072.0  969.0  110.0  26.0  0.0  125.0  0.0  0.0  28.0   \n",
      "1        0  100.0  1072.0  969.0  110.0  26.0  0.0  125.0  0.0  0.0  28.0   \n",
      "2        0  100.0  1072.0  969.0  110.0  26.0  0.0  125.0  0.0  0.0  28.0   \n",
      "3        0  100.0  1072.0  969.0  110.0  26.0  0.0  125.0  0.0  0.0  28.0   \n",
      "4        0  100.0  1072.0  969.0  110.0  26.0  0.0  125.0  0.0  0.0  28.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0   170    80  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1   217   214  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   289   163  8.526701e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   413   269  8.549627e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   414   200  8.526702e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(123611, 19)\n",
      "   mcsteps      1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  497.0  1053.0  957.0  110.0  31.0  0.0  138.0  0.0  0.0  28.0   \n",
      "1        0  497.0  1053.0  957.0  110.0  31.0  0.0  138.0  0.0  0.0  28.0   \n",
      "2        0  497.0  1053.0  957.0  110.0  31.0  0.0  138.0  0.0  0.0  28.0   \n",
      "3        0  497.0  1053.0  957.0  110.0  31.0  0.0  138.0  0.0  0.0  28.0   \n",
      "4        0  497.0  1053.0  957.0  110.0  31.0  0.0  138.0  0.0  0.0  28.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0    58   260  8.795903e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1   127   278  8.526748e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   320    86  8.556866e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   426   147  8.526957e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   389    77  8.527442e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(174195, 19)\n",
      "   mcsteps      1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  998.0  1059.0  954.0  110.0  24.0  0.0  141.0  0.0  0.0  28.0   \n",
      "1        0  998.0  1059.0  954.0  110.0  24.0  0.0  141.0  0.0  0.0  28.0   \n",
      "2        0  998.0  1059.0  954.0  110.0  24.0  0.0  141.0  0.0  0.0  28.0   \n",
      "3        0  998.0  1059.0  954.0  110.0  24.0  0.0  141.0  0.0  0.0  28.0   \n",
      "4        0  998.0  1059.0  954.0  110.0  24.0  0.0  141.0  0.0  0.0  28.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0   419   318  8.526879e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1   153   257  8.535946e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   202    53  8.527231e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   302    59  8.536351e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   121   130  8.528870e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(274522, 19)\n",
      "   mcsteps       1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  1988.0  1064.0  980.0  110.0  29.0  0.0  110.0  0.0  0.0  28.0   \n",
      "1        0  1988.0  1064.0  980.0  110.0  29.0  0.0  110.0  0.0  0.0  28.0   \n",
      "2        0  1988.0  1064.0  980.0  110.0  29.0  0.0  110.0  0.0  0.0  28.0   \n",
      "3        0  1988.0  1064.0  980.0  110.0  29.0  0.0  110.0  0.0  0.0  28.0   \n",
      "4        0  1988.0  1064.0  980.0  110.0  29.0  0.0  110.0  0.0  0.0  28.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0   262   196  9.054917e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1    92   109  8.679527e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   411   113  9.585515e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   413    95  8.532265e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   107   181  8.527046e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(371380, 19)\n",
      "   mcsteps       1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  2958.0  1065.0  984.0  109.0  30.0  0.0  108.0  0.0  0.0  29.0   \n",
      "1        0  2958.0  1065.0  984.0  109.0  30.0  0.0  108.0  0.0  0.0  29.0   \n",
      "2        0  2958.0  1065.0  984.0  109.0  30.0  0.0  108.0  0.0  0.0  29.0   \n",
      "3        0  2958.0  1065.0  984.0  109.0  30.0  0.0  108.0  0.0  0.0  29.0   \n",
      "4        0  2958.0  1065.0  984.0  109.0  30.0  0.0  108.0  0.0  0.0  29.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0   242   356  8.625129e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1    92   241  8.638730e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   228   230  8.541914e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3    89   420  8.535352e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "4   214   406  8.612076e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "(469718, 19)\n",
      "   mcsteps       1       2      3      4     5    6      7    8    9    10  \\\n",
      "0        0  3929.0  1062.0  974.0  110.0  30.0  0.0  115.0  0.0  0.0  28.0   \n",
      "1        0  3929.0  1062.0  974.0  110.0  30.0  0.0  115.0  0.0  0.0  28.0   \n",
      "2        0  3929.0  1062.0  974.0  110.0  30.0  0.0  115.0  0.0  0.0  28.0   \n",
      "3        0  3929.0  1062.0  974.0  110.0  30.0  0.0  115.0  0.0  0.0  28.0   \n",
      "4        0  3929.0  1062.0  974.0  110.0  30.0  0.0  115.0  0.0  0.0  28.0   \n",
      "\n",
      "   xCOM  yCOM           il8  il1  il6  il10  tnf  tgf  \n",
      "0   180   177  8.555751e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "1   425   119  8.671062e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "2   300   170  8.722691e-10  0.0  0.0   0.0  0.0  0.0  \n",
      "3   200   400  1.141548e-09  0.0  0.0   0.0  0.0  0.0  \n",
      "4   385   107  1.167906e-09  0.0  0.0   0.0  0.0  0.0  \n"
     ]
    }
   ],
   "source": [
    "data_frames = []\n",
    "for i in range(1, 8):\n",
    "    file_path = f'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data/combined_data_batch_{i}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.drop(columns=['zCOM'], inplace=True)  # Drop the 'zCOM' column\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    data_frames.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    " class Model(tf.keras.Model):\n",
    "    def __init__(self, model_path, train_mode=True, input_dim=19, lstm_size=500, batch_size=4, e_learning_rate=1e-5):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.train_mode = train_mode\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.e_learning_rate = e_learning_rate\n",
    "\n",
    "        #define LSTM layer\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(units=self.lstm_size, return_sequences=True)\n",
    "\n",
    "        #define output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, activation=None)\n",
    "        # Define reshape layer\n",
    "        self.reshape_layer = tf.keras.layers.Reshape((500, 500))  # Reshape to match spatial dimensions\n",
    "\n",
    "        #define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.e_learning_rate)\n",
    "\n",
    "        #define MSE as loss function\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Reshape inputs to add sequence length dimension\n",
    "        inputs = tf.expand_dims(inputs, axis=1)\n",
    "        \n",
    "        # Input shape: (batch_size, sequence_length, input_dim)\n",
    "        x = self.lstm_layer(inputs)\n",
    "        # Output shape: (batch_size, sequence_length, lstm_size)\n",
    "        output = self.output_layer(x)\n",
    "        # Output shape: (batch_size, sequence_length, 2) - 2 for outputs\n",
    "        \n",
    "        # Reshape output to match spatial dimensions\n",
    "        output = self.reshape_layer(output)\n",
    "        return output\n",
    "\n",
    "    def train_step(self, xtrain, ytrain):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(xtrain, training=True)\n",
    "            loss = self.loss_fn(ytrain, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_set, valid_set, maxEpoch=10):\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        \n",
    "        for epoch in range(maxEpoch):\n",
    "            train_loss = self.train_step(x_train, y_train)\n",
    "            valid_loss = self.loss_fn(y_valid, self(x_valid, training=False))\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df1 = data_frames[0]\n",
    "    \n",
    "    #convert the df to numpy array and cast the data to float\n",
    "    results = df1.to_numpy(dtype='float')\n",
    "\n",
    "    #define input indices and output indices\n",
    "    input_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "    output_indices = [11, 12] \n",
    "\n",
    "   # Split data into train and valid sets\n",
    "    train_size = int(len(results) * 0.9)\n",
    "    train_features = results[:train_size, input_indices]\n",
    "    train_targets = results[:train_size, output_indices]\n",
    "    valid_features = results[train_size:, input_indices]\n",
    "    valid_targets = results[train_size:, output_indices]\n",
    "\n",
    "    # Create train and valid sets with input features and targets\n",
    "    train_set = (train_features, train_targets)\n",
    "    valid_set = (valid_features, valid_targets)\n",
    "\n",
    "    #initialize and train the model\n",
    "    mymodel = Model(model_path=\"saved_model\")\n",
    "    mymodel.train(train_set, valid_set, maxEpoch=500)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of        mcsteps     1       2      3      4     5    6      7    8      9  \\\n",
      "0            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "1            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "2            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "3            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "4            0  10.0  1072.0  972.0  109.0  25.0  0.0  121.0  0.0    0.0   \n",
      "...        ...   ...     ...    ...    ...   ...  ...    ...  ...    ...   \n",
      "74753  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74754  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74755  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74756  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "74757  1000000  10.0    14.0   84.0  105.0   2.0  0.0   38.0  0.0  307.0   \n",
      "\n",
      "          10  xCOM  yCOM           il8           il1           il6  \\\n",
      "0       29.0    84   376  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "1       29.0   432   181  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "2       29.0   409   105  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "3       29.0   247   394  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "4       29.0   132   141  8.526701e-10  0.000000e+00  0.000000e+00   \n",
      "...      ...   ...   ...           ...           ...           ...   \n",
      "74753  163.0   468   284 -5.413690e-13  2.505693e-11  3.018151e-10   \n",
      "74754  163.0   475   433 -1.276491e-12  7.168744e-11  1.215652e-11   \n",
      "74755  163.0   425     3 -6.611300e-13  4.721322e-11  6.383543e-12   \n",
      "74756  163.0     2   475 -1.999419e-12  1.093847e-10  6.594112e-12   \n",
      "74757  163.0   262   493 -6.534840e-13  3.342928e-11  7.318865e-12   \n",
      "\n",
      "               il10           tnf           tgf  \n",
      "0      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...             ...           ...           ...  \n",
      "74753  7.795065e-11  7.162435e-11  5.866973e-10  \n",
      "74754  1.033143e-14  2.937644e-13  7.248822e-10  \n",
      "74755  8.737277e-16  4.176781e-10  4.689090e-10  \n",
      "74756  1.326679e-15  6.911885e-14  6.668235e-10  \n",
      "74757  2.317959e-15  5.880362e-14  4.089780e-10  \n",
      "\n",
      "[74758 rows x 19 columns]>\n",
      "(74758, 19)\n"
     ]
    }
   ],
   "source": [
    "print(df1.info)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = data_frames[6]\n",
    "# separate features (X) and target variables (y)\n",
    "X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "y = df7[['xCOM', 'yCOM']].values\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# reshape input data to 3D for LSTM input\n",
    "# the input shape is samples, timesteps, features\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "custom_optimizer = Adam(learning_rate=0.001) # learning rate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))  # dropout regularization\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.2)) \n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(2))  # output layer with 2 neurons for xCOM and yCOM\n",
    "model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "\n",
    "# train\n",
    "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=128, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "# evaluate\n",
    "loss = model.evaluate(X_test_reshaped, y_test)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "predictions = model.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1652/1652 [==============================] - 35s 19ms/step - loss: 30146.3242 - val_loss: 15526.3955\n",
      "Epoch 2/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15247.3193 - val_loss: 15029.4688\n",
      "Epoch 3/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15185.9131 - val_loss: 15030.0566\n",
      "Epoch 4/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15185.9229 - val_loss: 15029.5264\n",
      "Epoch 5/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.8623 - val_loss: 15032.1777\n",
      "Epoch 6/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.4238 - val_loss: 15029.5645\n",
      "Epoch 7/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.5322 - val_loss: 15030.3740\n",
      "Epoch 8/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.2100 - val_loss: 15030.1895\n",
      "Epoch 9/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.0947 - val_loss: 15030.2539\n",
      "Epoch 10/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15186.3643 - val_loss: 15029.8496\n",
      "Epoch 11/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.1641 - val_loss: 15030.0615\n",
      "Epoch 12/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15186.2090 - val_loss: 15030.4922\n",
      "Epoch 13/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15182.5498 - val_loss: 14987.3828\n",
      "Epoch 14/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15113.1279 - val_loss: 14935.3975\n",
      "Epoch 15/50\n",
      "1652/1652 [==============================] - 32s 19ms/step - loss: 15067.1807 - val_loss: 14901.7793\n",
      "Epoch 16/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 15041.3330 - val_loss: 14876.9375\n",
      "Epoch 17/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 15012.5322 - val_loss: 14842.7705\n",
      "Epoch 18/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14972.0625 - val_loss: 14795.4277\n",
      "Epoch 19/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14913.0234 - val_loss: 14731.7500\n",
      "Epoch 20/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14837.8486 - val_loss: 14634.2949\n",
      "Epoch 21/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14755.2070 - val_loss: 14575.2725\n",
      "Epoch 22/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14656.5449 - val_loss: 14467.7188\n",
      "Epoch 23/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14548.2734 - val_loss: 14363.7637\n",
      "Epoch 24/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 14430.5869 - val_loss: 14245.7949\n",
      "Epoch 25/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14313.6738 - val_loss: 14145.7783\n",
      "Epoch 26/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 14189.7178 - val_loss: 14020.6602\n",
      "Epoch 27/50\n",
      "1652/1652 [==============================] - 32s 19ms/step - loss: 14069.5820 - val_loss: 13924.1328\n",
      "Epoch 28/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13949.5957 - val_loss: 13827.8369\n",
      "Epoch 29/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13835.5068 - val_loss: 13719.3887\n",
      "Epoch 30/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13727.9609 - val_loss: 13661.8389\n",
      "Epoch 31/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13621.7939 - val_loss: 13553.1826\n",
      "Epoch 32/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13517.4229 - val_loss: 13445.6133\n",
      "Epoch 33/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13419.6719 - val_loss: 13353.1934\n",
      "Epoch 34/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 13324.2412 - val_loss: 13286.1846\n",
      "Epoch 35/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13228.3594 - val_loss: 13214.7236\n",
      "Epoch 36/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 13142.9346 - val_loss: 13153.7041\n",
      "Epoch 37/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 13050.9316 - val_loss: 13041.5938\n",
      "Epoch 38/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12972.0928 - val_loss: 12988.6113\n",
      "Epoch 39/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12886.8721 - val_loss: 12911.6611\n",
      "Epoch 40/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12809.9844 - val_loss: 12877.7666\n",
      "Epoch 41/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12739.7412 - val_loss: 12804.1787\n",
      "Epoch 42/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12663.8633 - val_loss: 12785.6348\n",
      "Epoch 43/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12595.9541 - val_loss: 12691.6367\n",
      "Epoch 44/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12527.5186 - val_loss: 12634.2559\n",
      "Epoch 45/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12458.5781 - val_loss: 12609.5020\n",
      "Epoch 46/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12393.8613 - val_loss: 12573.6035\n",
      "Epoch 47/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12335.5117 - val_loss: 12545.0205\n",
      "Epoch 48/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12273.2949 - val_loss: 12480.3174\n",
      "Epoch 49/50\n",
      "1652/1652 [==============================] - 31s 19ms/step - loss: 12213.8906 - val_loss: 12414.9141\n",
      "Epoch 50/50\n",
      "1652/1652 [==============================] - 30s 18ms/step - loss: 12154.4375 - val_loss: 12387.5439\n",
      "1468/1468 [==============================] - 3s 2ms/step - loss: 12387.5430\n",
      "Test Loss: 12387.54296875\n",
      "1468/1468 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "class MyLSTMModel:\n",
    "    def __init__(self, data_frames):\n",
    "        self.data_frames = data_frames\n",
    "        self.X_train_reshaped = None\n",
    "        self.X_test_reshaped = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self, test_size=0.1):\n",
    "        df7 = self.data_frames[6]\n",
    "        X = df7.drop(columns=['xCOM', 'yCOM']).values\n",
    "        y = df7[['xCOM', 'yCOM']].values\n",
    "\n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        self.X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        self.X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "    def build_model(self, optimizer='adam'):\n",
    "        custom_optimizer = Adam(learning_rate=0.001) if optimizer == 'adam' else optimizer\n",
    "\n",
    "        self.model = Sequential([\n",
    "            LSTM(256, input_shape=(self.X_train_reshaped.shape[1], self.X_train_reshaped.shape[2]), return_sequences=True),\n",
    "            LSTM(256, return_sequences=True),\n",
    "            LSTM(256),\n",
    "            Dense(2)\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=custom_optimizer)\n",
    "\n",
    "    def train_model(self, epochs=50, batch_size=256):\n",
    "        self.model.fit(self.X_train_reshaped, self.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.X_test_reshaped, self.y_test))\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        loss = self.model.evaluate(self.X_test_reshaped, self.y_test)\n",
    "        print('Test Loss:', loss)\n",
    "\n",
    "    def predict(self):\n",
    "        predictions = self.model.predict(self.X_test_reshaped)\n",
    "        return predictions\n",
    "\n",
    "data_frames\n",
    "lstm_model = MyLSTMModel(data_frames)\n",
    "lstm_model.prepare_data()\n",
    "lstm_model.build_model()\n",
    "lstm_model.train_model()\n",
    "lstm_model.evaluate_model()\n",
    "predictions = lstm_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
