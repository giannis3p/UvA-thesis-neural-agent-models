{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.constraints import MinMaxNorm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert .txt to .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations_txt/S8\"\n",
    "output_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations/S8\"\n",
    "\n",
    "# create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# iterate through files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            # construct input and output file paths\n",
    "            input_filepath = os.path.join(input_folder, filename)\n",
    "            output_filepath = os.path.join(output_folder, os.path.splitext(filename)[0] + \".csv\")\n",
    "\n",
    "            # read txt file and remove leading and trailing quotation marks from each line\n",
    "            with open(input_filepath, 'r') as file:\n",
    "                lines = [line.strip().strip('\"') for line in file.readlines()]\n",
    "\n",
    "            # convert to df and save as csv\n",
    "            df = pd.DataFrame([line.split(',') for line in lines])\n",
    "            df.to_csv(output_filepath, index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "            print(f\"Converted {input_filepath} to {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {input_filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge data in batches to create the 'merged_data' folder containing all the available data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellcounts_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts'\n",
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "cellcounts_batch_size = 8\n",
    "cytokine_batch_size = 101\n",
    "\n",
    "for batch_index in range(1, cellcounts_batch_size + 1):\n",
    "    print(f\"Processing batch {batch_index}\")\n",
    "    cellcounts_file = os.path.join(cellcounts_folder, f'cellcount S{batch_index}.csv')\n",
    "    print(\"Cellcounts file:\", cellcounts_file)\n",
    "    cytokine_subfolder = f'S{batch_index}'\n",
    "    cytokine_folder = os.path.join(base_cytokine_folder, cytokine_subfolder)\n",
    "    df_cellcounts_batch = pd.read_csv(cellcounts_file)\n",
    "    print(\"Cellcounts batch shape:\", df_cellcounts_batch.shape)\n",
    "    df_cytokine_batch = pd.DataFrame()\n",
    "    for cytokine_file in os.listdir(cytokine_folder):\n",
    "        if cytokine_file.endswith('.csv'):\n",
    "            file_path = os.path.join(cytokine_folder, cytokine_file)\n",
    "            df_cytokine = pd.read_csv(file_path)\n",
    "            df_cytokine_batch = pd.concat([df_cytokine_batch, df_cytokine], ignore_index=True)\n",
    "\n",
    "    print(\"Cytokine batch shape:\", df_cytokine_batch.shape)\n",
    "\n",
    "    merged_data = pd.merge(df_cellcounts_batch, df_cytokine_batch, on='mcsteps')\n",
    "    print(\"Merged data shape:\", merged_data.shape)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f'combined_data_batch_{batch_index}.csv')\n",
    "    merged_data.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved merged data to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat the cytokines data to create the 'all_data' folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/all_data'\n",
    "\n",
    "def create_simulation_subfolders(output_folder, simulation_names):\n",
    "    for sim_name in simulation_names:\n",
    "        sim_folder = os.path.join(output_folder, sim_name)\n",
    "        os.makedirs(sim_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def concat_files_in_folder(folder_path):\n",
    "    all_data = pd.DataFrame()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_data = pd.concat([all_data, df])\n",
    "    return all_data\n",
    "\n",
    "def sort_by_mcsteps(data):\n",
    "    return data.sort_values(by='mcsteps')\n",
    "\n",
    "simulation_names = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8']\n",
    "create_simulation_subfolders(output_folder, simulation_names)\n",
    "\n",
    "for sim_name in simulation_names:\n",
    "    sim_folder_path = os.path.join(output_folder, sim_name)\n",
    "    sim_data = concat_files_in_folder(os.path.join(base_cytokine_folder, sim_name))\n",
    "    sorted_data = sort_by_mcsteps(sim_data)\n",
    "    output_file_path = os.path.join(sim_folder_path, f'{sim_name}_data_sorted.csv')\n",
    "    sorted_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data to cover all initializations (df1 to df8) and drop zCOM column as we have 2D spatial data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/all_data\"\n",
    "\n",
    "def load_and_drop_zCOM(folder_path):\n",
    "    df = pd.concat([pd.read_csv(os.path.join(folder_path, file)) for file in os.listdir(folder_path) if file.endswith('.csv')])\n",
    "    df.drop(columns=['zCOM'], inplace=True)\n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "for i in range(1, 9):\n",
    "    sim_folder_path = os.path.join(output_folder, f'S{i}')\n",
    "    df = load_and_drop_zCOM(sim_folder_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "df1, df2, df3, df4, df5, df6, df7, df8 = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change 'mcsteps' to 'time' and print dataframe to make sure it works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time  xCOM  yCOM           il8           il1           il6  \\\n",
      "0          0   147   116  8.603181e-10  0.000000e+00  0.000000e+00   \n",
      "1          0   251   364  9.141505e-10  0.000000e+00  0.000000e+00   \n",
      "2          0   279   112  9.350631e-10  0.000000e+00  0.000000e+00   \n",
      "3          0    83   288  9.888261e-10  0.000000e+00  0.000000e+00   \n",
      "4          0   171   371  1.105985e-09  0.000000e+00  0.000000e+00   \n",
      "...      ...   ...   ...           ...           ...           ...   \n",
      "569270   100   172   192  1.493359e-08  1.195303e-11  3.136481e-27   \n",
      "569271   100   141   107  1.232809e-08  1.575511e-11  3.546226e-15   \n",
      "569272   100    81   257  1.258248e-08  1.729580e-11  1.002741e-11   \n",
      "569273   100   127   264  1.046952e-08  9.687031e-12  1.480615e-14   \n",
      "569274   100   298   363  1.142387e-08  7.674055e-12  5.401783e-25   \n",
      "\n",
      "                il10           tnf           tgf  \n",
      "0       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4       0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...              ...           ...           ...  \n",
      "569270  0.000000e+00  5.448368e-14  3.486646e-11  \n",
      "569271  4.554151e-23  9.951106e-22  2.055369e-10  \n",
      "569272  1.976363e-14  5.256890e-17  9.035102e-11  \n",
      "569273  1.299817e-22  2.521331e-17  8.664754e-11  \n",
      "569274  1.290000e-43  7.946154e-12  9.634385e-12  \n",
      "\n",
      "[569275 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_10104\\2445232496.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df8.drop(columns=['mcsteps'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df8['time'] = (df8['mcsteps'] / 10000).astype(int)\n",
    "df8 = df8[['time'] + [col for col in df8.columns if col != 'time']]\n",
    "df8.drop(columns=['mcsteps'], inplace=True)\n",
    "print(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest values for each cytokine:\n",
      "il8    -1.802096e-11\n",
      "il1     0.000000e+00\n",
      "il6     0.000000e+00\n",
      "il10    0.000000e+00\n",
      "tnf     0.000000e+00\n",
      "tgf     0.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "Largest values for each cytokine:\n",
      "il8     1.817857e-08\n",
      "il1     7.833237e-09\n",
      "il6     6.361151e-09\n",
      "il10    1.409022e-09\n",
      "tnf     5.365540e-08\n",
      "tgf     1.111927e-08\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select columns containing cytokine data\n",
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# Find the smallest values for each cytokine\n",
    "smallest_values = df8[cytokine_columns].min()\n",
    "\n",
    "# Find the largest values for each cytokine\n",
    "largest_values = df8[cytokine_columns].max()\n",
    "\n",
    "print(\"Smallest values for each cytokine:\")\n",
    "print(smallest_values)\n",
    "\n",
    "print(\"\\nLargest values for each cytokine:\")\n",
    "print(largest_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an array for each unique mcsteps value, should taken a couple minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# get unique time values\n",
    "unique_time = df8['time'].unique()\n",
    "\n",
    "arrays = {}\n",
    "\n",
    "# iterate over unique time values\n",
    "for time in unique_time:\n",
    "    # filter data for current value of time\n",
    "    df_time = df8[df8['time'] == time]\n",
    "    \n",
    "    # initialize 500x500 array for current value of time\n",
    "    array = np.zeros((500, 500, len(cytokines)))\n",
    "    \n",
    "    # iterate over rows in filtered df\n",
    "    for index, row in df_time.iterrows():\n",
    "        # get X and Y coordinates\n",
    "        x = int(row['xCOM'])\n",
    "        y = int(row['yCOM'])\n",
    "        \n",
    "        # get cytokine concentrations\n",
    "        concentrations = row[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "        \n",
    "        # assign cytokine concentrations to corresponding position in array\n",
    "        array[x, y] = concentrations\n",
    "    \n",
    "    # store array for current value of time\n",
    "    arrays[time] = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print to make sure array works as intended**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays: 101\n",
      "Shape of the array: (500, 500, 6)\n",
      "Value at position (356,200): [1.1378244e-08 2.0458439e-11 5.3863960e-15 5.2224680e-23 7.6039260e-18\n",
      " 1.2120631e-11]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of arrays:\", len(arrays))\n",
    "array = arrays[100]\n",
    "print(\"Shape of the array:\", array.shape)\n",
    "print(\"Value at position (356,200):\", array[356,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create input_sequences and output_values for the LSTM to use in order to be able to predict output_values from input_sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "output_values = []\n",
    "\n",
    "# convert dictionary values to a list of arrays\n",
    "arrays_list = [arrays[key] for key in sorted(arrays.keys())]\n",
    "\n",
    "# convert 'arrays' list to numpy array\n",
    "arrays_np = np.array(arrays_list)\n",
    "\n",
    "for i in range(len(arrays_np) - sequence_length):\n",
    "    input_seq = arrays_np[i:i+sequence_length]  # input sequence of arrays\n",
    "    output_val = arrays_np[i+sequence_length]   # array at next time step\n",
    "    \n",
    "    input_sequences.append(input_seq)\n",
    "    output_values.append(output_val)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_values = np.array(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input_sequences has a shape of (91, 10, 500, 500, 6), which means we have 91 samples, each consisting of 10 arrays of shape (500, 500, 6).**\n",
    "\n",
    "**output_values has a shape of (91, 500, 500, 6), indicating that each sample has an output array of shape (500, 500, 6).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_sequences.shape)\n",
    "print(output_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv to vtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_0.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_1.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_2.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_3.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_4.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_5.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_6.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_7.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_8.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_9.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_10.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_11.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_12.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_13.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_14.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_15.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_16.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_17.vtk\n",
      "Successfully wrote C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\\timestep_18.vtk\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import vtk\n",
    "import os\n",
    "\n",
    "def csv_to_vtk_per_timestep(csv_file, output_dir):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        required_columns = ['timestep', 'X', 'Y']\n",
    "        if not all(column in df.columns for column in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain columns: {', '.join(required_columns)}\")\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        timesteps = df['timestep'].unique()\n",
    "        feature_names = [col for col in df.columns if col not in ['timestep', 'X', 'Y']]\n",
    "\n",
    "        for timestep in timesteps:\n",
    "            timestep_df = df[df['timestep'] == timestep]\n",
    "\n",
    "            # Create VTK StructuredPoints object\n",
    "            structured_points = vtk.vtkStructuredPoints()\n",
    "            structured_points.SetDimensions(50, 50, 1)  # Assuming 50x50 grid\n",
    "            structured_points.SetSpacing(1, 1, 1)\n",
    "            structured_points.SetOrigin(0, 0, 0)\n",
    "\n",
    "            # Add point data (features)\n",
    "            point_data = structured_points.GetPointData()\n",
    "            for feature in feature_names:\n",
    "                feature_array = vtk.vtkFloatArray()\n",
    "                feature_array.SetName(feature)\n",
    "                feature_array.SetNumberOfComponents(1)\n",
    "                feature_array.SetNumberOfTuples(50 * 50)  # Assuming 50x50 grid\n",
    "\n",
    "                for index, value in enumerate(timestep_df[feature]):\n",
    "                    feature_array.SetValue(index, value)\n",
    "\n",
    "                point_data.AddArray(feature_array)\n",
    "\n",
    "            # Write the data to a VTK file\n",
    "            vtk_file_path = os.path.join(output_dir, f\"timestep_{timestep}.vtk\")\n",
    "            writer = vtk.vtkStructuredPointsWriter()\n",
    "            writer.SetFileName(vtk_file_path)\n",
    "            writer.SetInputData(structured_points)\n",
    "            writer.Write()\n",
    "            print(f\"Successfully wrote {vtk_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage\n",
    "csv_file_path = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/PINN(82-100hrs).csv\"\n",
    "output_dir = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/vtk_outputs\"\n",
    "csv_to_vtk_per_timestep(csv_file_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
