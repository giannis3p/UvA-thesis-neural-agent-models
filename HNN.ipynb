{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.layers import Dropout,  TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import HeNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mcsteps  xCOM  yCOM           il8  il1  il6  il10  tnf  tgf\n",
      "0        0    33    22  3.750408e-07  0.0  0.0   0.0  0.0  0.0\n",
      "1        0    11    32  4.793328e-09  0.0  0.0   0.0  0.0  0.0\n",
      "2        0    30    35  8.997935e-09  0.0  0.0   0.0  0.0  0.0\n",
      "3        0    17    40  1.781103e-08  0.0  0.0   0.0  0.0  0.0\n",
      "4        0    21    23  4.789621e-09  0.0  0.0   0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "sorted_concatenated_csv = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/test_data_concat/sorted_concatenated_data.csv\"\n",
    "data = pd.read_csv(sorted_concatenated_csv)\n",
    "data.drop(columns=['zCOM'], inplace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time  xCOM  yCOM           il8           il1           il6  \\\n",
      "0         0    33    22  3.750408e-07  0.000000e+00  0.000000e+00   \n",
      "1         0    11    32  4.793328e-09  0.000000e+00  0.000000e+00   \n",
      "2         0    30    35  8.997935e-09  0.000000e+00  0.000000e+00   \n",
      "3         0    17    40  1.781103e-08  0.000000e+00  0.000000e+00   \n",
      "4         0    21    23  4.789621e-09  0.000000e+00  0.000000e+00   \n",
      "...     ...   ...   ...           ...           ...           ...   \n",
      "86155   100    49    35 -8.886183e-11  6.343394e-10  1.206925e-08   \n",
      "86156   100    36     4  1.334245e-07  1.665160e-07  1.305691e-08   \n",
      "86157   100    49    44 -1.233990e-10  1.178848e-09  1.615457e-07   \n",
      "86158   100    45    10  3.257988e-07  4.403049e-08  1.749928e-07   \n",
      "86159   100    35    38  2.107191e-05  9.170692e-08  5.693712e-19   \n",
      "\n",
      "               il10           tnf           tgf  \n",
      "0      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "1      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "2      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "3      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "4      0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "...             ...           ...           ...  \n",
      "86155  3.819835e-10  1.979543e-10  4.400244e-06  \n",
      "86156  4.469681e-10  3.720676e-07  7.833499e-06  \n",
      "86157  3.270504e-08  9.659641e-08  6.022260e-06  \n",
      "86158  3.550616e-08  1.001522e-07  6.226569e-06  \n",
      "86159  5.103635e-25  4.620498e-09  2.838544e-12  \n",
      "\n",
      "[86160 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data['time'] = (data['mcsteps'] / 10000).astype(int)\n",
    "data = data[['time'] + [col for col in data.columns if col != 'time']]\n",
    "data.drop(columns=['mcsteps'], inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest values for each cytokine:\n",
      "il8    -9.336660e-09\n",
      "il1     0.000000e+00\n",
      "il6     0.000000e+00\n",
      "il10    0.000000e+00\n",
      "tnf     0.000000e+00\n",
      "tgf     0.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "Largest values for each cytokine:\n",
      "il8     2.745434e-05\n",
      "il1     8.067287e-06\n",
      "il6     8.228592e-07\n",
      "il10    1.480818e-07\n",
      "tnf     1.316345e-05\n",
      "tgf     1.271714e-05\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "smallest_values = data[cytokine_columns].min()\n",
    "largest_values = data[cytokine_columns].max()\n",
    "\n",
    "print(\"Smallest values for each cytokine:\")\n",
    "print(smallest_values)\n",
    "print(\"\\nLargest values for each cytokine:\")\n",
    "print(largest_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative values replaced with 0 in 'il8': 7147\n",
      "Number of negative values replaced with 0 in 'il1': 0\n",
      "Number of negative values replaced with 0 in 'il6': 0\n",
      "Number of negative values replaced with 0 in 'il10': 0\n",
      "Number of negative values replaced with 0 in 'tnf': 0\n",
      "Number of negative values replaced with 0 in 'tgf': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n",
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n",
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n",
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n",
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n",
      "C:\\Users\\Ioannis\\AppData\\Local\\Temp\\ipykernel_16152\\3859886154.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[data < 0] = 0\n"
     ]
    }
   ],
   "source": [
    "def replace_negative_with_zero(data):\n",
    "    num_negative_values = (data < 0).sum().sum()\n",
    "    data[data < 0] = 0\n",
    "\n",
    "    return num_negative_values\n",
    "\n",
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "for col in cytokine_columns:\n",
    "    num_negatives = replace_negative_with_zero(data[col])\n",
    "    print(f\"Number of negative values replaced with 0 in '{col}': {num_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# get unique time values\n",
    "unique_time = data['time'].unique()\n",
    "\n",
    "arrays = {}\n",
    "\n",
    "# iterate over unique time values\n",
    "for time in unique_time:\n",
    "    # filter data for current value of time\n",
    "    data_time = data[data['time'] == time]\n",
    "    \n",
    "    # initialize 50x50x6 array for current value of time\n",
    "    array = np.zeros((50, 50, len(cytokines)))\n",
    "    \n",
    "    # get X and Y coordinates\n",
    "    x = data_time['xCOM'].astype(int)\n",
    "    y = data_time['yCOM'].astype(int)\n",
    "    \n",
    "    # get cytokine concentrations\n",
    "    concentrations = data_time[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "    \n",
    "    # assign cytokine concentrations to corresponding position in array\n",
    "    array[x, y, :] = concentrations\n",
    "    \n",
    "    # store array for current value of time\n",
    "    arrays[time] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays: 101\n",
      "Shape of the array: (50, 50, 6)\n",
      "Value at position (0,45): [0.00000e+00 6.79662e-23 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of arrays:\", len(arrays))\n",
    "array = arrays[1]\n",
    "print(\"Shape of the array:\", array.shape)\n",
    "print(\"Value at position (0,45):\", array[0,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "output_values = []\n",
    "\n",
    "# convert dictionary values to a list of arrays\n",
    "arrays_list = [arrays[key] for key in sorted(arrays.keys())]\n",
    "\n",
    "# convert 'arrays' list to numpy array\n",
    "arrays_np = np.array(arrays_list)\n",
    "\n",
    "for i in range(len(arrays_np) - sequence_length):\n",
    "    input_seq = arrays_np[i:i+sequence_length]  # input sequence of arrays\n",
    "    output_val = arrays_np[i+sequence_length]   # array at next time step\n",
    "    \n",
    "    input_sequences.append(input_seq)\n",
    "    output_values.append(output_val)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_values = np.array(output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 10, 50, 50, 6)\n",
      "(91, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences.shape)\n",
    "print(output_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hamiltonian Neural Network architecture\n",
    "def create_hamiltonian_network(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Reshape((input_shape[0], -1))(x)\n",
    "    x = LSTM(units=64, return_sequences=True, kernel_regularizer='l2')(x)\n",
    "    x = LSTM(units=64)(x)\n",
    "    H = Dense(units=1, activation='linear')(x)\n",
    "    model = Model(inputs, H)\n",
    "    return model\n",
    "\n",
    "# Define custom loss function for Hamiltonian dynamics\n",
    "@tf.function\n",
    "def hamiltonian_loss(y_true, y_pred):\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    y_true_flat = tf.reshape(y_true, [batch_size, -1])\n",
    "    half_size = tf.shape(y_true_flat)[1] // 2\n",
    "    \n",
    "    q = y_true_flat[:, :half_size]\n",
    "    p = y_true_flat[:, half_size:]\n",
    "\n",
    "    q = tf.reshape(q, [batch_size, 50, 50, 3])\n",
    "    p = tf.reshape(p, [batch_size, 50, 50, 3])\n",
    "\n",
    "    print(\"q shape:\", q.shape)\n",
    "    print(\"p shape:\", p.shape)\n",
    "    print(\"y_pred shape:\", y_pred.shape)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([q, p])\n",
    "        # Compute Hamiltonian explicitly from q and p for tracking\n",
    "        H = model(tf.concat([q, p], axis=-1))\n",
    "\n",
    "    dH_dq = tape.gradient(H, q)\n",
    "    dH_dp = tape.gradient(H, p)\n",
    "\n",
    "    print(\"dH_dq:\", dH_dq)\n",
    "    print(\"dH_dp:\", dH_dp)\n",
    "\n",
    "    if dH_dq is None or dH_dp is None:\n",
    "        raise RuntimeError(\"Gradient calculation failed\")\n",
    "\n",
    "    dq_dt = dH_dp\n",
    "    dp_dt = -dH_dq\n",
    "\n",
    "    q_pred = q + dq_dt\n",
    "    p_pred = p + dp_dt\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return K.mean(K.square(q_pred - q) + K.square(p_pred - p))\n",
    "\n",
    "# Define metrics\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return 1 - SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-4\n",
    "    if epoch < 500:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-4\n",
    "\n",
    "def average_relative_rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_pred - y_true) / K.clip(K.abs(y_true), K.epsilon(), None))))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    abs_diff = K.abs(y_true - y_pred)\n",
    "    threshold = 0.2 * y_true\n",
    "    accurate_predictions = K.less_equal(abs_diff, threshold)\n",
    "    accuracy = K.mean(accurate_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Prepare the data\n",
    "train_size = int(0.7 * input_sequences.shape[0])\n",
    "val_size = int(0.1 * input_sequences.shape[0])\n",
    "test_size = input_sequences.shape[0] - train_size - val_size\n",
    "\n",
    "X_train = input_sequences[:train_size]\n",
    "X_val = input_sequences[train_size:train_size + val_size]\n",
    "X_test = input_sequences[train_size + val_size:]\n",
    "y_train = output_values[:train_size]\n",
    "y_val = output_values[train_size:train_size + val_size]\n",
    "y_test = output_values[train_size + val_size:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Create the Hamiltonian Neural Network model\n",
    "input_shape = (sequence_length, 50, 50, 6)\n",
    "model = create_hamiltonian_network(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "initial_lr = 1e-4\n",
    "model.compile(optimizer=Adam(learning_rate=initial_lr), loss=hamiltonian_loss, metrics=[r_squared, 'mape', accuracy, average_relative_rmse, 'msle', 'mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32, callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
