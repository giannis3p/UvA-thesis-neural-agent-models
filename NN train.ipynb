{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations\\\\S1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Read corresponding cytokine concentration files\u001b[39;00m\n\u001b[0;32m     26\u001b[0m cytokine_batch_files \u001b[38;5;241m=\u001b[39m cytokine_files[batch_index \u001b[38;5;241m*\u001b[39m cytokine_batch_size : (batch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m cytokine_batch_size]\n\u001b[1;32m---> 27\u001b[0m cytokine_data_batch \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m cytokine_batch_files]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Merge data based on 'mcstep' column\u001b[39;00m\n\u001b[0;32m     30\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m cellcounts_data[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming one cellcounts file per batch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Read corresponding cytokine concentration files\u001b[39;00m\n\u001b[0;32m     26\u001b[0m cytokine_batch_files \u001b[38;5;241m=\u001b[39m cytokine_files[batch_index \u001b[38;5;241m*\u001b[39m cytokine_batch_size : (batch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m cytokine_batch_size]\n\u001b[1;32m---> 27\u001b[0m cytokine_data_batch \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m cytokine_batch_files]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Merge data based on 'mcstep' column\u001b[39;00m\n\u001b[0;32m     30\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m cellcounts_data[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming one cellcounts file per batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Program Files\\Python38\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations\\\\S1'"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "cellcounts_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcounts'\n",
    "cytokine_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/concentrations'\n",
    "output_folder = 'C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/merged_data'\n",
    "\n",
    "\n",
    "# Get list of cellcounts files\n",
    "cellcounts_files = sorted([os.path.join(cellcounts_folder, file) for file in os.listdir(cellcounts_folder)])\n",
    "\n",
    "# Get list of cytokine concentration files\n",
    "cytokine_files = sorted([os.path.join(cytokine_folder, file) for file in os.listdir(cytokine_folder)])\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1  # 1 cellcounts file per batch\n",
    "cytokine_batch_size = 10  # 10 cytokine concentration files per batch\n",
    "\n",
    "# Iterate over batches\n",
    "for batch_index in range(len(cellcounts_files)):\n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = min(start_index + batch_size, len(cellcounts_files))\n",
    "    \n",
    "    # Read cellcounts data\n",
    "    cellcounts_data = [pd.read_csv(cellcounts_files[i]) for i in range(start_index, end_index)]\n",
    "    \n",
    "    # Read corresponding cytokine concentration files\n",
    "    cytokine_batch_files = cytokine_files[batch_index * cytokine_batch_size : (batch_index + 1) * cytokine_batch_size]\n",
    "    cytokine_data_batch = [pd.read_csv(file) for file in cytokine_batch_files]\n",
    "    \n",
    "    # Merge data based on 'mcstep' column\n",
    "    merged_data = cellcounts_data[0]  # Assuming one cellcounts file per batch\n",
    "    for cytokine_data in cytokine_data_batch:\n",
    "        merged_data = pd.merge(merged_data, cytokine_data, on='mcstep', how='inner')\n",
    "    \n",
    "    # Save merged data to output folder\n",
    "    output_filename = os.path.join(output_folder, f'merged_data_batch_{batch_index}.csv')\n",
    "    merged_data.to_csv(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csvs\n",
    "df1 = pd.read_csv(\"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/cellcount S6.csv\")\n",
    "df2 = pd.read_csv(\"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/mean_concentration.csv\")\n",
    "df2.rename(columns={'meanconcen': 'mcsteps'}, inplace=True)\n",
    "\n",
    "#merge dfs using mcsteps as common column\n",
    "csvfile1 = pd.merge(df1, df2, on='mcsteps')\n",
    "\n",
    "#save to new csv file\n",
    "csvfile1.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the DataFrame:\n",
      "   mcsteps  xCOM  yCOM  zCOM           il8  il1  il6  il10  tnf  tgf\n",
      "0        0   242   356     0  8.625129e-10  0.0  0.0   0.0  0.0  0.0\n",
      "1        0    92   241     0  8.638730e-10  0.0  0.0   0.0  0.0  0.0\n",
      "2        0   228   230     0  8.541914e-10  0.0  0.0   0.0  0.0  0.0\n",
      "3        0    89   420     0  8.535352e-10  0.0  0.0   0.0  0.0  0.0\n",
      "4        0   214   406     0  8.612076e-10  0.0  0.0   0.0  0.0  0.0\n",
      "\n",
      "Information about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15748 entries, 0 to 15747\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   mcsteps  15748 non-null  int64  \n",
      " 1   xCOM     15748 non-null  int64  \n",
      " 2   yCOM     15748 non-null  int64  \n",
      " 3   zCOM     15748 non-null  int64  \n",
      " 4   il8      15748 non-null  float64\n",
      " 5   il1      15748 non-null  float64\n",
      " 6   il6      15748 non-null  float64\n",
      " 7   il10     15748 non-null  float64\n",
      " 8   tnf      15748 non-null  float64\n",
      " 9   tgf      15748 non-null  float64\n",
      "dtypes: float64(6), int64(4)\n",
      "memory usage: 1.2 MB\n",
      "None\n",
      "\n",
      "Descriptive statistics of the features:\n",
      "            mcsteps          xCOM          yCOM     zCOM           il8  \\\n",
      "count  15748.000000  15748.000000  15748.000000  15748.0  1.574800e+04   \n",
      "mean    9935.864872    251.018034    250.723330      0.0  6.898005e-10   \n",
      "std     8151.870299    146.888293    147.289303      0.0  6.280247e-10   \n",
      "min        0.000000      0.000000      0.000000      0.0 -1.462712e-11   \n",
      "25%        0.000000    122.000000    120.000000      0.0  1.394289e-19   \n",
      "50%    10000.000000    252.000000    252.000000      0.0  8.635876e-10   \n",
      "75%    20000.000000    378.000000    380.000000      0.0  1.261510e-09   \n",
      "max    20000.000000    499.000000    499.000000      0.0  2.885214e-09   \n",
      "\n",
      "                il1           il6          il10           tnf           tgf  \n",
      "count  1.574800e+04  1.574800e+04  1.574800e+04  1.574800e+04  1.574800e+04  \n",
      "mean   1.297356e-11  2.638715e-12  5.480255e-13  2.415728e-11  1.897884e-13  \n",
      "std    1.707382e-10  9.554072e-11  1.984580e-11  3.136840e-10  2.381673e-11  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "max    3.719442e-09  3.462526e-09  7.191883e-10  8.190403e-09  2.988786e-09  \n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "df2 = pd.read_csv(\"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/conc0.csv\")\n",
    "df3 = pd.read_csv(\"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/conc1.csv\")\n",
    "df4 = pd.read_csv(\"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/conc2.csv\")\n",
    "\n",
    "csvfile1 = pd.concat([df2, df3, df4], ignore_index=True)\n",
    "#print(csvfile1)\n",
    "csvfile1.to_csv(\"testdata.csv\", index=False)\n",
    "\n",
    "#display the first few rows of df\n",
    "print(csvfile1.head())\n",
    "print(csvfile1.info())\n",
    "print(csvfile1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, model_path, train_mode=True, input_dim=15, lstm_size=256, batch_size=10, e_learning_rate=1e-4):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.train_mode = train_mode\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.e_learning_rate = e_learning_rate\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(units=self.lstm_size, return_sequences=True)\n",
    "\n",
    "        # Define output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, activation=None)\n",
    "\n",
    "        # Define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.e_learning_rate)\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Input shape: (batch_size, sequence_length, input_dim)\n",
    "        x = self.lstm_layer(inputs)\n",
    "        # Output shape: (batch_size, sequence_length, lstm_size)\n",
    "        output = self.output_layer(x)\n",
    "        # Output shape: (batch_size, sequence_length, 2) - 2 for tnf_mean and tgf_mean\n",
    "        return output\n",
    "\n",
    "    def train_step(self, xtrain, ytrain):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(xtrain, training=True)\n",
    "            loss = self.loss_fn(ytrain, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred):\n",
    "    # Calculate mean squared error\n",
    "       mse = tf.keras.losses.MeanSquaredError()\n",
    "       mse_loss = mse(y_true, y_pred)\n",
    "\n",
    "    # Calculate mean squared error of the true values\n",
    "       mse_true = tf.reduce_mean(tf.square(y_true))\n",
    "\n",
    "    # Calculate accuracy\n",
    "       accuracy = 1.0 - mse_loss / mse_true\n",
    "       return accuracy\n",
    "\n",
    "    def train(self, train_set, valid_set, maxEpoch=10):\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        for epoch in range(maxEpoch):\n",
    "            train_loss = self.train_step(x_train, y_train)\n",
    "            valid_pred = self(x_valid, training=False)\n",
    "            valid_loss = self.loss_fn(y_valid, valid_pred)\n",
    "            y_valid = tf.cast(y_valid, tf.float32)  # Cast y_valid to tf.float32\n",
    "            accuracy = self.calculate_accuracy(y_valid, valid_pred)\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csvfile1 = pd.read_csv('C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/combined_data.csv', skiprows=[0])\n",
    "    csvfile1 = csvfile1.iloc[:, :-6]\n",
    "\n",
    "    # Convert the DataFrame to a NumPy array and cast the data to float\n",
    "    results = csvfile1.to_numpy(dtype='float')\n",
    "\n",
    "    # Define input indices and output indices\n",
    "    input_indices = list(range(15))\n",
    "    output_indices = [15, 16]  # Assuming 15 is tnf_mean and 16 is tgf_mean\n",
    "\n",
    "   # Split data into train and valid sets\n",
    "    train_size = int(len(results) * 0.9)\n",
    "    train_features = results[:train_size, input_indices]\n",
    "    train_targets = results[:train_size, output_indices]\n",
    "    valid_features = results[train_size:, input_indices]\n",
    "    valid_targets = results[train_size:, output_indices]\n",
    "\n",
    "# Reshape input features to include a sequence length dimension\n",
    "    sequence_length = train_features.shape[0]  # Assuming each row is a sequence\n",
    "    train_features = np.expand_dims(train_features, axis=1)  # Add sequence length dimension\n",
    "    valid_features = np.expand_dims(valid_features, axis=1)  # Add sequence length dimension\n",
    "\n",
    "# Create train and valid sets with input features and targets\n",
    "    train_set = (train_features, train_targets)\n",
    "    valid_set = (valid_features, valid_targets)\n",
    "    \n",
    "\n",
    "    # Initialize and train the model\n",
    "    mymodel = Model(model_path=\"saved_model\")\n",
    "    mymodel.train(train_set, valid_set, maxEpoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 83971.2421875, Valid Loss: 93946.4453125\n",
      "Epoch 2, Train Loss: 83970.6796875, Valid Loss: 93945.921875\n",
      "Epoch 3, Train Loss: 83970.140625, Valid Loss: 93945.4453125\n",
      "Epoch 4, Train Loss: 83969.5859375, Valid Loss: 93944.9609375\n",
      "Epoch 5, Train Loss: 83969.046875, Valid Loss: 93944.53125\n",
      "Epoch 6, Train Loss: 83968.5078125, Valid Loss: 93944.0859375\n",
      "Epoch 7, Train Loss: 83967.96875, Valid Loss: 93943.671875\n",
      "Epoch 8, Train Loss: 83967.421875, Valid Loss: 93943.28125\n",
      "Epoch 9, Train Loss: 83966.890625, Valid Loss: 93942.921875\n",
      "Epoch 10, Train Loss: 83966.3515625, Valid Loss: 93942.5703125\n",
      "Epoch 11, Train Loss: 83965.8203125, Valid Loss: 93942.21875\n",
      "Epoch 12, Train Loss: 83965.2890625, Valid Loss: 93941.8984375\n",
      "Epoch 13, Train Loss: 83964.765625, Valid Loss: 93941.5703125\n",
      "Epoch 14, Train Loss: 83964.2265625, Valid Loss: 93941.265625\n",
      "Epoch 15, Train Loss: 83963.6953125, Valid Loss: 93940.9609375\n",
      "Epoch 16, Train Loss: 83963.1796875, Valid Loss: 93940.65625\n",
      "Epoch 17, Train Loss: 83962.640625, Valid Loss: 93940.359375\n",
      "Epoch 18, Train Loss: 83962.125, Valid Loss: 93940.0625\n",
      "Epoch 19, Train Loss: 83961.5859375, Valid Loss: 93939.765625\n",
      "Epoch 20, Train Loss: 83961.0625, Valid Loss: 93939.484375\n",
      "Epoch 21, Train Loss: 83960.546875, Valid Loss: 93939.203125\n",
      "Epoch 22, Train Loss: 83960.0, Valid Loss: 93938.8984375\n",
      "Epoch 23, Train Loss: 83959.4765625, Valid Loss: 93938.609375\n",
      "Epoch 24, Train Loss: 83958.9609375, Valid Loss: 93938.3359375\n",
      "Epoch 25, Train Loss: 83958.4375, Valid Loss: 93938.0390625\n",
      "Epoch 26, Train Loss: 83957.90625, Valid Loss: 93937.75\n",
      "Epoch 27, Train Loss: 83957.3828125, Valid Loss: 93937.453125\n",
      "Epoch 28, Train Loss: 83956.84375, Valid Loss: 93937.1640625\n",
      "Epoch 29, Train Loss: 83956.3203125, Valid Loss: 93936.8671875\n",
      "Epoch 30, Train Loss: 83955.78125, Valid Loss: 93936.5703125\n",
      "Epoch 31, Train Loss: 83955.265625, Valid Loss: 93936.28125\n",
      "Epoch 32, Train Loss: 83954.7421875, Valid Loss: 93935.9765625\n",
      "Epoch 33, Train Loss: 83954.203125, Valid Loss: 93935.6796875\n",
      "Epoch 34, Train Loss: 83953.671875, Valid Loss: 93935.3828125\n",
      "Epoch 35, Train Loss: 83953.1328125, Valid Loss: 93935.0859375\n",
      "Epoch 36, Train Loss: 83952.6015625, Valid Loss: 93934.7890625\n",
      "Epoch 37, Train Loss: 83952.0703125, Valid Loss: 93934.484375\n",
      "Epoch 38, Train Loss: 83951.5234375, Valid Loss: 93934.1875\n",
      "Epoch 39, Train Loss: 83951.0, Valid Loss: 93933.8671875\n",
      "Epoch 40, Train Loss: 83950.4609375, Valid Loss: 93933.578125\n",
      "Epoch 41, Train Loss: 83949.90625, Valid Loss: 93933.28125\n",
      "Epoch 42, Train Loss: 83949.375, Valid Loss: 93932.9765625\n",
      "Epoch 43, Train Loss: 83948.828125, Valid Loss: 93932.6796875\n",
      "Epoch 44, Train Loss: 83948.28125, Valid Loss: 93932.3671875\n",
      "Epoch 45, Train Loss: 83947.7421875, Valid Loss: 93932.0703125\n",
      "Epoch 46, Train Loss: 83947.1953125, Valid Loss: 93931.765625\n",
      "Epoch 47, Train Loss: 83946.640625, Valid Loss: 93931.453125\n",
      "Epoch 48, Train Loss: 83946.09375, Valid Loss: 93931.171875\n",
      "Epoch 49, Train Loss: 83945.5390625, Valid Loss: 93930.859375\n",
      "Epoch 50, Train Loss: 83944.9921875, Valid Loss: 93930.5703125\n",
      "Epoch 51, Train Loss: 83944.4453125, Valid Loss: 93930.25\n",
      "Epoch 52, Train Loss: 83943.890625, Valid Loss: 93929.96875\n",
      "Epoch 53, Train Loss: 83943.3359375, Valid Loss: 93929.6796875\n",
      "Epoch 54, Train Loss: 83942.7734375, Valid Loss: 93929.3828125\n",
      "Epoch 55, Train Loss: 83942.234375, Valid Loss: 93929.09375\n",
      "Epoch 56, Train Loss: 83941.6875, Valid Loss: 93928.8046875\n",
      "Epoch 57, Train Loss: 83941.125, Valid Loss: 93928.5234375\n",
      "Epoch 58, Train Loss: 83940.5703125, Valid Loss: 93928.2421875\n",
      "Epoch 59, Train Loss: 83940.0390625, Valid Loss: 93927.96875\n",
      "Epoch 60, Train Loss: 83939.484375, Valid Loss: 93927.671875\n",
      "Epoch 61, Train Loss: 83938.921875, Valid Loss: 93927.3984375\n",
      "Epoch 62, Train Loss: 83938.359375, Valid Loss: 93927.140625\n",
      "Epoch 63, Train Loss: 83937.8203125, Valid Loss: 93926.859375\n",
      "Epoch 64, Train Loss: 83937.2734375, Valid Loss: 93926.5859375\n",
      "Epoch 65, Train Loss: 83936.7265625, Valid Loss: 93926.3046875\n",
      "Epoch 66, Train Loss: 83936.1796875, Valid Loss: 93926.03125\n",
      "Epoch 67, Train Loss: 83935.6328125, Valid Loss: 93925.7734375\n",
      "Epoch 68, Train Loss: 83935.09375, Valid Loss: 93925.5\n",
      "Epoch 69, Train Loss: 83934.546875, Valid Loss: 93925.234375\n",
      "Epoch 70, Train Loss: 83934.0078125, Valid Loss: 93924.953125\n",
      "Epoch 71, Train Loss: 83933.46875, Valid Loss: 93924.6875\n",
      "Epoch 72, Train Loss: 83932.9375, Valid Loss: 93924.4296875\n",
      "Epoch 73, Train Loss: 83932.3984375, Valid Loss: 93924.1640625\n",
      "Epoch 74, Train Loss: 83931.8515625, Valid Loss: 93923.8984375\n",
      "Epoch 75, Train Loss: 83931.3203125, Valid Loss: 93923.625\n",
      "Epoch 76, Train Loss: 83930.796875, Valid Loss: 93923.359375\n",
      "Epoch 77, Train Loss: 83930.265625, Valid Loss: 93923.0859375\n",
      "Epoch 78, Train Loss: 83929.734375, Valid Loss: 93922.828125\n",
      "Epoch 79, Train Loss: 83929.203125, Valid Loss: 93922.5546875\n",
      "Epoch 80, Train Loss: 83928.6875, Valid Loss: 93922.296875\n",
      "Epoch 81, Train Loss: 83928.15625, Valid Loss: 93922.03125\n",
      "Epoch 82, Train Loss: 83927.640625, Valid Loss: 93921.75\n",
      "Epoch 83, Train Loss: 83927.125, Valid Loss: 93921.4921875\n",
      "Epoch 84, Train Loss: 83926.6015625, Valid Loss: 93921.21875\n",
      "Epoch 85, Train Loss: 83926.078125, Valid Loss: 93920.953125\n",
      "Epoch 86, Train Loss: 83925.578125, Valid Loss: 93920.671875\n",
      "Epoch 87, Train Loss: 83925.0703125, Valid Loss: 93920.421875\n",
      "Epoch 88, Train Loss: 83924.5546875, Valid Loss: 93920.1484375\n",
      "Epoch 89, Train Loss: 83924.0390625, Valid Loss: 93919.8671875\n",
      "Epoch 90, Train Loss: 83923.5234375, Valid Loss: 93919.59375\n",
      "Epoch 91, Train Loss: 83923.03125, Valid Loss: 93919.328125\n",
      "Epoch 92, Train Loss: 83922.5234375, Valid Loss: 93919.0546875\n",
      "Epoch 93, Train Loss: 83922.0078125, Valid Loss: 93918.8046875\n",
      "Epoch 94, Train Loss: 83921.5078125, Valid Loss: 93918.5234375\n",
      "Epoch 95, Train Loss: 83921.0, Valid Loss: 93918.25\n",
      "Epoch 96, Train Loss: 83920.4921875, Valid Loss: 93917.96875\n",
      "Epoch 97, Train Loss: 83919.9921875, Valid Loss: 93917.7109375\n",
      "Epoch 98, Train Loss: 83919.4921875, Valid Loss: 93917.4453125\n",
      "Epoch 99, Train Loss: 83918.9921875, Valid Loss: 93917.15625\n",
      "Epoch 100, Train Loss: 83918.484375, Valid Loss: 93916.890625\n",
      "Epoch 101, Train Loss: 83917.984375, Valid Loss: 93916.609375\n",
      "Epoch 102, Train Loss: 83917.4765625, Valid Loss: 93916.328125\n",
      "Epoch 103, Train Loss: 83916.984375, Valid Loss: 93916.0390625\n",
      "Epoch 104, Train Loss: 83916.484375, Valid Loss: 93915.7421875\n",
      "Epoch 105, Train Loss: 83915.9765625, Valid Loss: 93915.4453125\n",
      "Epoch 106, Train Loss: 83915.4765625, Valid Loss: 93915.109375\n",
      "Epoch 107, Train Loss: 83914.96875, Valid Loss: 93914.7578125\n",
      "Epoch 108, Train Loss: 83914.4609375, Valid Loss: 93914.3828125\n",
      "Epoch 109, Train Loss: 83913.953125, Valid Loss: 93913.9765625\n",
      "Epoch 110, Train Loss: 83913.4453125, Valid Loss: 93913.5234375\n",
      "Epoch 111, Train Loss: 83912.9296875, Valid Loss: 93913.03125\n",
      "Epoch 112, Train Loss: 83912.3984375, Valid Loss: 93912.5078125\n",
      "Epoch 113, Train Loss: 83911.8828125, Valid Loss: 93911.953125\n",
      "Epoch 114, Train Loss: 83911.3515625, Valid Loss: 93911.359375\n",
      "Epoch 115, Train Loss: 83910.828125, Valid Loss: 93910.734375\n",
      "Epoch 116, Train Loss: 83910.296875, Valid Loss: 93910.0859375\n",
      "Epoch 117, Train Loss: 83909.7578125, Valid Loss: 93909.421875\n",
      "Epoch 118, Train Loss: 83909.1953125, Valid Loss: 93908.7265625\n",
      "Epoch 119, Train Loss: 83908.6484375, Valid Loss: 93908.015625\n",
      "Epoch 120, Train Loss: 83908.0859375, Valid Loss: 93907.2734375\n",
      "Epoch 121, Train Loss: 83907.515625, Valid Loss: 93906.515625\n",
      "Epoch 122, Train Loss: 83906.953125, Valid Loss: 93905.75\n",
      "Epoch 123, Train Loss: 83906.3671875, Valid Loss: 93904.9296875\n",
      "Epoch 124, Train Loss: 83905.796875, Valid Loss: 93904.125\n",
      "Epoch 125, Train Loss: 83905.1953125, Valid Loss: 93903.2734375\n",
      "Epoch 126, Train Loss: 83904.5859375, Valid Loss: 93902.40625\n",
      "Epoch 127, Train Loss: 83903.9609375, Valid Loss: 93901.5234375\n",
      "Epoch 128, Train Loss: 83903.3359375, Valid Loss: 93900.640625\n",
      "Epoch 129, Train Loss: 83902.6875, Valid Loss: 93899.7265625\n",
      "Epoch 130, Train Loss: 83902.046875, Valid Loss: 93898.796875\n",
      "Epoch 131, Train Loss: 83901.375, Valid Loss: 93897.9140625\n",
      "Epoch 132, Train Loss: 83900.7109375, Valid Loss: 93897.03125\n",
      "Epoch 133, Train Loss: 83900.015625, Valid Loss: 93896.1796875\n",
      "Epoch 134, Train Loss: 83899.3359375, Valid Loss: 93895.34375\n",
      "Epoch 135, Train Loss: 83898.640625, Valid Loss: 93894.5546875\n",
      "Epoch 136, Train Loss: 83897.9375, Valid Loss: 93893.7890625\n",
      "Epoch 137, Train Loss: 83897.234375, Valid Loss: 93893.0234375\n",
      "Epoch 138, Train Loss: 83896.5390625, Valid Loss: 93892.2890625\n",
      "Epoch 139, Train Loss: 83895.8125, Valid Loss: 93891.5234375\n",
      "Epoch 140, Train Loss: 83895.109375, Valid Loss: 93890.796875\n",
      "Epoch 141, Train Loss: 83894.375, Valid Loss: 93890.046875\n",
      "Epoch 142, Train Loss: 83893.640625, Valid Loss: 93889.3203125\n",
      "Epoch 143, Train Loss: 83892.90625, Valid Loss: 93888.6171875\n",
      "Epoch 144, Train Loss: 83892.171875, Valid Loss: 93887.921875\n",
      "Epoch 145, Train Loss: 83891.4375, Valid Loss: 93887.2265625\n",
      "Epoch 146, Train Loss: 83890.6875, Valid Loss: 93886.5234375\n",
      "Epoch 147, Train Loss: 83889.9453125, Valid Loss: 93885.8359375\n",
      "Epoch 148, Train Loss: 83889.2109375, Valid Loss: 93885.1484375\n",
      "Epoch 149, Train Loss: 83888.46875, Valid Loss: 93884.4375\n",
      "Epoch 150, Train Loss: 83887.7109375, Valid Loss: 93883.734375\n",
      "Epoch 151, Train Loss: 83886.96875, Valid Loss: 93883.046875\n",
      "Epoch 152, Train Loss: 83886.203125, Valid Loss: 93882.3671875\n",
      "Epoch 153, Train Loss: 83885.4609375, Valid Loss: 93881.6875\n",
      "Epoch 154, Train Loss: 83884.7109375, Valid Loss: 93881.0234375\n",
      "Epoch 155, Train Loss: 83883.96875, Valid Loss: 93880.375\n",
      "Epoch 156, Train Loss: 83883.21875, Valid Loss: 93879.734375\n",
      "Epoch 157, Train Loss: 83882.4765625, Valid Loss: 93879.0703125\n",
      "Epoch 158, Train Loss: 83881.7265625, Valid Loss: 93878.4453125\n",
      "Epoch 159, Train Loss: 83880.984375, Valid Loss: 93877.8125\n",
      "Epoch 160, Train Loss: 83880.2265625, Valid Loss: 93877.1640625\n",
      "Epoch 161, Train Loss: 83879.484375, Valid Loss: 93876.5390625\n",
      "Epoch 162, Train Loss: 83878.7421875, Valid Loss: 93875.8828125\n",
      "Epoch 163, Train Loss: 83877.984375, Valid Loss: 93875.25\n",
      "Epoch 164, Train Loss: 83877.2421875, Valid Loss: 93874.640625\n",
      "Epoch 165, Train Loss: 83876.515625, Valid Loss: 93874.0078125\n",
      "Epoch 166, Train Loss: 83875.78125, Valid Loss: 93873.390625\n",
      "Epoch 167, Train Loss: 83875.046875, Valid Loss: 93872.7734375\n",
      "Epoch 168, Train Loss: 83874.3203125, Valid Loss: 93872.1484375\n",
      "Epoch 169, Train Loss: 83873.5859375, Valid Loss: 93871.5546875\n",
      "Epoch 170, Train Loss: 83872.859375, Valid Loss: 93870.9375\n",
      "Epoch 171, Train Loss: 83872.1328125, Valid Loss: 93870.3125\n",
      "Epoch 172, Train Loss: 83871.40625, Valid Loss: 93869.6875\n",
      "Epoch 173, Train Loss: 83870.6796875, Valid Loss: 93869.0546875\n",
      "Epoch 174, Train Loss: 83869.9453125, Valid Loss: 93868.40625\n",
      "Epoch 175, Train Loss: 83869.2265625, Valid Loss: 93867.7265625\n",
      "Epoch 176, Train Loss: 83868.4921875, Valid Loss: 93867.0390625\n",
      "Epoch 177, Train Loss: 83867.765625, Valid Loss: 93866.3515625\n",
      "Epoch 178, Train Loss: 83867.0625, Valid Loss: 93865.6640625\n",
      "Epoch 179, Train Loss: 83866.3515625, Valid Loss: 93864.9296875\n",
      "Epoch 180, Train Loss: 83865.640625, Valid Loss: 93864.21875\n",
      "Epoch 181, Train Loss: 83864.9453125, Valid Loss: 93863.46875\n",
      "Epoch 182, Train Loss: 83864.25, Valid Loss: 93862.7421875\n",
      "Epoch 183, Train Loss: 83863.5703125, Valid Loss: 93862.0234375\n",
      "Epoch 184, Train Loss: 83862.8828125, Valid Loss: 93861.2890625\n",
      "Epoch 185, Train Loss: 83862.203125, Valid Loss: 93860.5625\n",
      "Epoch 186, Train Loss: 83861.53125, Valid Loss: 93859.8515625\n",
      "Epoch 187, Train Loss: 83860.859375, Valid Loss: 93859.15625\n",
      "Epoch 188, Train Loss: 83860.21875, Valid Loss: 93858.4609375\n",
      "Epoch 189, Train Loss: 83859.5625, Valid Loss: 93857.78125\n",
      "Epoch 190, Train Loss: 83858.921875, Valid Loss: 93857.1328125\n",
      "Epoch 191, Train Loss: 83858.2890625, Valid Loss: 93856.484375\n",
      "Epoch 192, Train Loss: 83857.6484375, Valid Loss: 93855.84375\n",
      "Epoch 193, Train Loss: 83857.03125, Valid Loss: 93855.2265625\n",
      "Epoch 194, Train Loss: 83856.4140625, Valid Loss: 93854.625\n",
      "Epoch 195, Train Loss: 83855.8125, Valid Loss: 93854.03125\n",
      "Epoch 196, Train Loss: 83855.1953125, Valid Loss: 93853.4375\n",
      "Epoch 197, Train Loss: 83854.6015625, Valid Loss: 93852.859375\n",
      "Epoch 198, Train Loss: 83853.9921875, Valid Loss: 93852.3046875\n",
      "Epoch 199, Train Loss: 83853.4140625, Valid Loss: 93851.7578125\n",
      "Epoch 200, Train Loss: 83852.8203125, Valid Loss: 93851.203125\n",
      "Epoch 201, Train Loss: 83852.2421875, Valid Loss: 93850.6796875\n",
      "Epoch 202, Train Loss: 83851.6640625, Valid Loss: 93850.1640625\n",
      "Epoch 203, Train Loss: 83851.0859375, Valid Loss: 93849.6640625\n",
      "Epoch 204, Train Loss: 83850.515625, Valid Loss: 93849.171875\n",
      "Epoch 205, Train Loss: 83849.9453125, Valid Loss: 93848.6875\n",
      "Epoch 206, Train Loss: 83849.3671875, Valid Loss: 93848.203125\n",
      "Epoch 207, Train Loss: 83848.8125, Valid Loss: 93847.7578125\n",
      "Epoch 208, Train Loss: 83848.2421875, Valid Loss: 93847.3125\n",
      "Epoch 209, Train Loss: 83847.6796875, Valid Loss: 93846.875\n",
      "Epoch 210, Train Loss: 83847.125, Valid Loss: 93846.4375\n",
      "Epoch 211, Train Loss: 83846.5703125, Valid Loss: 93846.0234375\n",
      "Epoch 212, Train Loss: 83846.015625, Valid Loss: 93845.6328125\n",
      "Epoch 213, Train Loss: 83845.46875, Valid Loss: 93845.21875\n",
      "Epoch 214, Train Loss: 83844.90625, Valid Loss: 93844.8359375\n",
      "Epoch 215, Train Loss: 83844.359375, Valid Loss: 93844.4609375\n",
      "Epoch 216, Train Loss: 83843.8046875, Valid Loss: 93844.1015625\n",
      "Epoch 217, Train Loss: 83843.2578125, Valid Loss: 93843.734375\n",
      "Epoch 218, Train Loss: 83842.703125, Valid Loss: 93843.3828125\n",
      "Epoch 219, Train Loss: 83842.15625, Valid Loss: 93843.0234375\n",
      "Epoch 220, Train Loss: 83841.6015625, Valid Loss: 93842.6796875\n",
      "Epoch 221, Train Loss: 83841.0546875, Valid Loss: 93842.3359375\n",
      "Epoch 222, Train Loss: 83840.5078125, Valid Loss: 93842.0078125\n",
      "Epoch 223, Train Loss: 83839.953125, Valid Loss: 93841.6640625\n",
      "Epoch 224, Train Loss: 83839.40625, Valid Loss: 93841.3359375\n",
      "Epoch 225, Train Loss: 83838.859375, Valid Loss: 93841.0234375\n",
      "Epoch 226, Train Loss: 83838.3046875, Valid Loss: 93840.6796875\n",
      "Epoch 227, Train Loss: 83837.7578125, Valid Loss: 93840.3671875\n",
      "Epoch 228, Train Loss: 83837.2109375, Valid Loss: 93840.0390625\n",
      "Epoch 229, Train Loss: 83836.6640625, Valid Loss: 93839.703125\n",
      "Epoch 230, Train Loss: 83836.109375, Valid Loss: 93839.3671875\n",
      "Epoch 231, Train Loss: 83835.5625, Valid Loss: 93839.0390625\n",
      "Epoch 232, Train Loss: 83835.0078125, Valid Loss: 93838.7109375\n",
      "Epoch 233, Train Loss: 83834.4609375, Valid Loss: 93838.390625\n",
      "Epoch 234, Train Loss: 83833.90625, Valid Loss: 93838.0546875\n",
      "Epoch 235, Train Loss: 83833.359375, Valid Loss: 93837.7265625\n",
      "Epoch 236, Train Loss: 83832.8046875, Valid Loss: 93837.3984375\n",
      "Epoch 237, Train Loss: 83832.25, Valid Loss: 93837.09375\n",
      "Epoch 238, Train Loss: 83831.6953125, Valid Loss: 93836.7734375\n",
      "Epoch 239, Train Loss: 83831.1328125, Valid Loss: 93836.453125\n",
      "Epoch 240, Train Loss: 83830.578125, Valid Loss: 93836.1328125\n",
      "Epoch 241, Train Loss: 83830.015625, Valid Loss: 93835.828125\n",
      "Epoch 242, Train Loss: 83829.4609375, Valid Loss: 93835.5234375\n",
      "Epoch 243, Train Loss: 83828.9140625, Valid Loss: 93835.2109375\n",
      "Epoch 244, Train Loss: 83828.34375, Valid Loss: 93834.9140625\n",
      "Epoch 245, Train Loss: 83827.796875, Valid Loss: 93834.625\n",
      "Epoch 246, Train Loss: 83827.2265625, Valid Loss: 93834.3203125\n",
      "Epoch 247, Train Loss: 83826.6640625, Valid Loss: 93834.015625\n",
      "Epoch 248, Train Loss: 83826.109375, Valid Loss: 93833.75\n",
      "Epoch 249, Train Loss: 83825.546875, Valid Loss: 93833.453125\n",
      "Epoch 250, Train Loss: 83824.9765625, Valid Loss: 93833.171875\n",
      "Epoch 251, Train Loss: 83824.4296875, Valid Loss: 93832.8828125\n",
      "Epoch 252, Train Loss: 83823.859375, Valid Loss: 93832.6015625\n",
      "Epoch 253, Train Loss: 83823.296875, Valid Loss: 93832.3125\n",
      "Epoch 254, Train Loss: 83822.7265625, Valid Loss: 93832.0390625\n",
      "Epoch 255, Train Loss: 83822.171875, Valid Loss: 93831.7578125\n",
      "Epoch 256, Train Loss: 83821.609375, Valid Loss: 93831.484375\n",
      "Epoch 257, Train Loss: 83821.046875, Valid Loss: 93831.1953125\n",
      "Epoch 258, Train Loss: 83820.4765625, Valid Loss: 93830.921875\n",
      "Epoch 259, Train Loss: 83819.9140625, Valid Loss: 93830.640625\n",
      "Epoch 260, Train Loss: 83819.3515625, Valid Loss: 93830.34375\n",
      "Epoch 261, Train Loss: 83818.796875, Valid Loss: 93830.078125\n",
      "Epoch 262, Train Loss: 83818.2265625, Valid Loss: 93829.7890625\n",
      "Epoch 263, Train Loss: 83817.6640625, Valid Loss: 93829.5234375\n",
      "Epoch 264, Train Loss: 83817.109375, Valid Loss: 93829.234375\n",
      "Epoch 265, Train Loss: 83816.5390625, Valid Loss: 93828.9453125\n",
      "Epoch 266, Train Loss: 83815.984375, Valid Loss: 93828.6796875\n",
      "Epoch 267, Train Loss: 83815.4296875, Valid Loss: 93828.3984375\n",
      "Epoch 268, Train Loss: 83814.859375, Valid Loss: 93828.1171875\n",
      "Epoch 269, Train Loss: 83814.296875, Valid Loss: 93827.84375\n",
      "Epoch 270, Train Loss: 83813.7421875, Valid Loss: 93827.5703125\n",
      "Epoch 271, Train Loss: 83813.1640625, Valid Loss: 93827.3046875\n",
      "Epoch 272, Train Loss: 83812.609375, Valid Loss: 93827.03125\n",
      "Epoch 273, Train Loss: 83812.046875, Valid Loss: 93826.75\n",
      "Epoch 274, Train Loss: 83811.484375, Valid Loss: 93826.4609375\n",
      "Epoch 275, Train Loss: 83810.9140625, Valid Loss: 93826.203125\n",
      "Epoch 276, Train Loss: 83810.359375, Valid Loss: 93825.9296875\n",
      "Epoch 277, Train Loss: 83809.796875, Valid Loss: 93825.65625\n",
      "Epoch 278, Train Loss: 83809.234375, Valid Loss: 93825.375\n",
      "Epoch 279, Train Loss: 83808.671875, Valid Loss: 93825.109375\n",
      "Epoch 280, Train Loss: 83808.1015625, Valid Loss: 93824.828125\n",
      "Epoch 281, Train Loss: 83807.53125, Valid Loss: 93824.5390625\n",
      "Epoch 282, Train Loss: 83806.984375, Valid Loss: 93824.25\n",
      "Epoch 283, Train Loss: 83806.3984375, Valid Loss: 93823.984375\n",
      "Epoch 284, Train Loss: 83805.8515625, Valid Loss: 93823.71875\n",
      "Epoch 285, Train Loss: 83805.28125, Valid Loss: 93823.4296875\n",
      "Epoch 286, Train Loss: 83804.71875, Valid Loss: 93823.15625\n",
      "Epoch 287, Train Loss: 83804.1484375, Valid Loss: 93822.8671875\n",
      "Epoch 288, Train Loss: 83803.5859375, Valid Loss: 93822.5859375\n",
      "Epoch 289, Train Loss: 83803.0234375, Valid Loss: 93822.3046875\n",
      "Epoch 290, Train Loss: 83802.453125, Valid Loss: 93822.015625\n",
      "Epoch 291, Train Loss: 83801.890625, Valid Loss: 93821.7421875\n",
      "Epoch 292, Train Loss: 83801.3125, Valid Loss: 93821.453125\n",
      "Epoch 293, Train Loss: 83800.75, Valid Loss: 93821.171875\n",
      "Epoch 294, Train Loss: 83800.1796875, Valid Loss: 93820.8984375\n",
      "Epoch 295, Train Loss: 83799.609375, Valid Loss: 93820.609375\n",
      "Epoch 296, Train Loss: 83799.046875, Valid Loss: 93820.3203125\n",
      "Epoch 297, Train Loss: 83798.484375, Valid Loss: 93820.046875\n",
      "Epoch 298, Train Loss: 83797.90625, Valid Loss: 93819.7578125\n",
      "Epoch 299, Train Loss: 83797.3359375, Valid Loss: 93819.484375\n",
      "Epoch 300, Train Loss: 83796.765625, Valid Loss: 93819.2109375\n",
      "Epoch 301, Train Loss: 83796.1953125, Valid Loss: 93818.9296875\n",
      "Epoch 302, Train Loss: 83795.6328125, Valid Loss: 93818.6640625\n",
      "Epoch 303, Train Loss: 83795.0546875, Valid Loss: 93818.375\n",
      "Epoch 304, Train Loss: 83794.4921875, Valid Loss: 93818.1171875\n",
      "Epoch 305, Train Loss: 83793.921875, Valid Loss: 93817.8359375\n",
      "Epoch 306, Train Loss: 83793.34375, Valid Loss: 93817.5625\n",
      "Epoch 307, Train Loss: 83792.7734375, Valid Loss: 93817.28125\n",
      "Epoch 308, Train Loss: 83792.1953125, Valid Loss: 93817.015625\n",
      "Epoch 309, Train Loss: 83791.625, Valid Loss: 93816.7578125\n",
      "Epoch 310, Train Loss: 83791.0625, Valid Loss: 93816.484375\n",
      "Epoch 311, Train Loss: 83790.484375, Valid Loss: 93816.1953125\n",
      "Epoch 312, Train Loss: 83789.9140625, Valid Loss: 93815.9453125\n",
      "Epoch 313, Train Loss: 83789.328125, Valid Loss: 93815.6796875\n",
      "Epoch 314, Train Loss: 83788.765625, Valid Loss: 93815.421875\n",
      "Epoch 315, Train Loss: 83788.1796875, Valid Loss: 93815.1484375\n",
      "Epoch 316, Train Loss: 83787.6015625, Valid Loss: 93814.875\n",
      "Epoch 317, Train Loss: 83787.0234375, Valid Loss: 93814.6171875\n",
      "Epoch 318, Train Loss: 83786.4453125, Valid Loss: 93814.359375\n",
      "Epoch 319, Train Loss: 83785.8671875, Valid Loss: 93814.09375\n",
      "Epoch 320, Train Loss: 83785.2890625, Valid Loss: 93813.828125\n",
      "Epoch 321, Train Loss: 83784.703125, Valid Loss: 93813.5625\n",
      "Epoch 322, Train Loss: 83784.125, Valid Loss: 93813.3046875\n",
      "Epoch 323, Train Loss: 83783.5390625, Valid Loss: 93813.0625\n",
      "Epoch 324, Train Loss: 83782.9609375, Valid Loss: 93812.7890625\n",
      "Epoch 325, Train Loss: 83782.375, Valid Loss: 93812.53125\n",
      "Epoch 326, Train Loss: 83781.7890625, Valid Loss: 93812.265625\n",
      "Epoch 327, Train Loss: 83781.2109375, Valid Loss: 93812.0078125\n",
      "Epoch 328, Train Loss: 83780.6171875, Valid Loss: 93811.7734375\n",
      "Epoch 329, Train Loss: 83780.0390625, Valid Loss: 93811.515625\n",
      "Epoch 330, Train Loss: 83779.453125, Valid Loss: 93811.2421875\n",
      "Epoch 331, Train Loss: 83778.859375, Valid Loss: 93811.0078125\n",
      "Epoch 332, Train Loss: 83778.2734375, Valid Loss: 93810.75\n",
      "Epoch 333, Train Loss: 83777.6796875, Valid Loss: 93810.4921875\n",
      "Epoch 334, Train Loss: 83777.0859375, Valid Loss: 93810.2265625\n",
      "Epoch 335, Train Loss: 83776.4921875, Valid Loss: 93809.9921875\n",
      "Epoch 336, Train Loss: 83775.8984375, Valid Loss: 93809.7265625\n",
      "Epoch 337, Train Loss: 83775.3046875, Valid Loss: 93809.484375\n",
      "Epoch 338, Train Loss: 83774.703125, Valid Loss: 93809.21875\n",
      "Epoch 339, Train Loss: 83774.09375, Valid Loss: 93808.984375\n",
      "Epoch 340, Train Loss: 83773.4921875, Valid Loss: 93808.734375\n",
      "Epoch 341, Train Loss: 83772.875, Valid Loss: 93808.484375\n",
      "Epoch 342, Train Loss: 83772.2578125, Valid Loss: 93808.25\n",
      "Epoch 343, Train Loss: 83771.6328125, Valid Loss: 93807.9765625\n",
      "Epoch 344, Train Loss: 83770.9921875, Valid Loss: 93807.7265625\n",
      "Epoch 345, Train Loss: 83770.3671875, Valid Loss: 93807.484375\n",
      "Epoch 346, Train Loss: 83769.7265625, Valid Loss: 93807.2421875\n",
      "Epoch 347, Train Loss: 83769.0703125, Valid Loss: 93806.9921875\n",
      "Epoch 348, Train Loss: 83768.40625, Valid Loss: 93806.7421875\n",
      "Epoch 349, Train Loss: 83767.734375, Valid Loss: 93806.4921875\n",
      "Epoch 350, Train Loss: 83767.0625, Valid Loss: 93806.234375\n",
      "Epoch 351, Train Loss: 83766.3828125, Valid Loss: 93805.9921875\n",
      "Epoch 352, Train Loss: 83765.703125, Valid Loss: 93805.7421875\n",
      "Epoch 353, Train Loss: 83764.9921875, Valid Loss: 93805.4921875\n",
      "Epoch 354, Train Loss: 83764.3046875, Valid Loss: 93805.25\n",
      "Epoch 355, Train Loss: 83763.59375, Valid Loss: 93805.0078125\n",
      "Epoch 356, Train Loss: 83762.875, Valid Loss: 93804.75\n",
      "Epoch 357, Train Loss: 83762.1484375, Valid Loss: 93804.5078125\n",
      "Epoch 358, Train Loss: 83761.421875, Valid Loss: 93804.265625\n",
      "Epoch 359, Train Loss: 83760.6875, Valid Loss: 93804.015625\n",
      "Epoch 360, Train Loss: 83759.9453125, Valid Loss: 93803.7734375\n",
      "Epoch 361, Train Loss: 83759.2109375, Valid Loss: 93803.5390625\n",
      "Epoch 362, Train Loss: 83758.46875, Valid Loss: 93803.28125\n",
      "Epoch 363, Train Loss: 83757.734375, Valid Loss: 93803.0390625\n",
      "Epoch 364, Train Loss: 83756.9921875, Valid Loss: 93802.7890625\n",
      "Epoch 365, Train Loss: 83756.25, Valid Loss: 93802.53125\n",
      "Epoch 366, Train Loss: 83755.5234375, Valid Loss: 93802.2890625\n",
      "Epoch 367, Train Loss: 83754.7890625, Valid Loss: 93802.046875\n",
      "Epoch 368, Train Loss: 83754.046875, Valid Loss: 93801.8046875\n",
      "Epoch 369, Train Loss: 83753.3203125, Valid Loss: 93801.5625\n",
      "Epoch 370, Train Loss: 83752.5859375, Valid Loss: 93801.3046875\n",
      "Epoch 371, Train Loss: 83751.859375, Valid Loss: 93801.0703125\n",
      "Epoch 372, Train Loss: 83751.1484375, Valid Loss: 93800.828125\n",
      "Epoch 373, Train Loss: 83750.421875, Valid Loss: 93800.6015625\n",
      "Epoch 374, Train Loss: 83749.703125, Valid Loss: 93800.3359375\n",
      "Epoch 375, Train Loss: 83748.9765625, Valid Loss: 93800.09375\n",
      "Epoch 376, Train Loss: 83748.2734375, Valid Loss: 93799.859375\n",
      "Epoch 377, Train Loss: 83747.546875, Valid Loss: 93799.6171875\n",
      "Epoch 378, Train Loss: 83746.828125, Valid Loss: 93799.375\n",
      "Epoch 379, Train Loss: 83746.1171875, Valid Loss: 93799.125\n",
      "Epoch 380, Train Loss: 83745.390625, Valid Loss: 93798.875\n",
      "Epoch 381, Train Loss: 83744.671875, Valid Loss: 93798.625\n",
      "Epoch 382, Train Loss: 83743.953125, Valid Loss: 93798.375\n",
      "Epoch 383, Train Loss: 83743.2421875, Valid Loss: 93798.125\n",
      "Epoch 384, Train Loss: 83742.5234375, Valid Loss: 93797.8359375\n",
      "Epoch 385, Train Loss: 83741.8046875, Valid Loss: 93797.53125\n",
      "Epoch 386, Train Loss: 83741.078125, Valid Loss: 93797.1953125\n",
      "Epoch 387, Train Loss: 83740.3515625, Valid Loss: 93796.8203125\n",
      "Epoch 388, Train Loss: 83739.640625, Valid Loss: 93796.375\n",
      "Epoch 389, Train Loss: 83738.8984375, Valid Loss: 93795.8671875\n",
      "Epoch 390, Train Loss: 83738.171875, Valid Loss: 93795.3125\n",
      "Epoch 391, Train Loss: 83737.4296875, Valid Loss: 93794.703125\n",
      "Epoch 392, Train Loss: 83736.6875, Valid Loss: 93794.0390625\n",
      "Epoch 393, Train Loss: 83735.9453125, Valid Loss: 93793.328125\n",
      "Epoch 394, Train Loss: 83735.1796875, Valid Loss: 93792.59375\n",
      "Epoch 395, Train Loss: 83734.4140625, Valid Loss: 93791.8203125\n",
      "Epoch 396, Train Loss: 83733.625, Valid Loss: 93791.0703125\n",
      "Epoch 397, Train Loss: 83732.8203125, Valid Loss: 93790.2734375\n",
      "Epoch 398, Train Loss: 83732.0078125, Valid Loss: 93789.5078125\n",
      "Epoch 399, Train Loss: 83731.171875, Valid Loss: 93788.7265625\n",
      "Epoch 400, Train Loss: 83730.3125, Valid Loss: 93787.9375\n",
      "Epoch 401, Train Loss: 83729.4453125, Valid Loss: 93787.1640625\n",
      "Epoch 402, Train Loss: 83728.5546875, Valid Loss: 93786.40625\n",
      "Epoch 403, Train Loss: 83727.6796875, Valid Loss: 93785.6796875\n",
      "Epoch 404, Train Loss: 83726.78125, Valid Loss: 93784.9765625\n",
      "Epoch 405, Train Loss: 83725.8828125, Valid Loss: 93784.3203125\n",
      "Epoch 406, Train Loss: 83724.9765625, Valid Loss: 93783.6796875\n",
      "Epoch 407, Train Loss: 83724.0625, Valid Loss: 93783.0625\n",
      "Epoch 408, Train Loss: 83723.1484375, Valid Loss: 93782.453125\n",
      "Epoch 409, Train Loss: 83722.2265625, Valid Loss: 93781.8671875\n",
      "Epoch 410, Train Loss: 83721.328125, Valid Loss: 93781.2734375\n",
      "Epoch 411, Train Loss: 83720.4140625, Valid Loss: 93780.7109375\n",
      "Epoch 412, Train Loss: 83719.484375, Valid Loss: 93780.109375\n",
      "Epoch 413, Train Loss: 83718.5703125, Valid Loss: 93779.5390625\n",
      "Epoch 414, Train Loss: 83717.65625, Valid Loss: 93778.953125\n",
      "Epoch 415, Train Loss: 83716.734375, Valid Loss: 93778.359375\n",
      "Epoch 416, Train Loss: 83715.8203125, Valid Loss: 93777.78125\n",
      "Epoch 417, Train Loss: 83714.921875, Valid Loss: 93777.2109375\n",
      "Epoch 418, Train Loss: 83714.015625, Valid Loss: 93776.6328125\n",
      "Epoch 419, Train Loss: 83713.1328125, Valid Loss: 93776.0859375\n",
      "Epoch 420, Train Loss: 83712.234375, Valid Loss: 93775.53125\n",
      "Epoch 421, Train Loss: 83711.3515625, Valid Loss: 93774.9921875\n",
      "Epoch 422, Train Loss: 83710.46875, Valid Loss: 93774.4453125\n",
      "Epoch 423, Train Loss: 83709.609375, Valid Loss: 93773.921875\n",
      "Epoch 424, Train Loss: 83708.7421875, Valid Loss: 93773.375\n",
      "Epoch 425, Train Loss: 83707.8671875, Valid Loss: 93772.8203125\n",
      "Epoch 426, Train Loss: 83707.0078125, Valid Loss: 93772.2578125\n",
      "Epoch 427, Train Loss: 83706.140625, Valid Loss: 93771.6796875\n",
      "Epoch 428, Train Loss: 83705.28125, Valid Loss: 93771.0703125\n",
      "Epoch 429, Train Loss: 83704.4296875, Valid Loss: 93770.4609375\n",
      "Epoch 430, Train Loss: 83703.59375, Valid Loss: 93769.84375\n",
      "Epoch 431, Train Loss: 83702.75, Valid Loss: 93769.1953125\n",
      "Epoch 432, Train Loss: 83701.921875, Valid Loss: 93768.5625\n",
      "Epoch 433, Train Loss: 83701.1171875, Valid Loss: 93767.890625\n",
      "Epoch 434, Train Loss: 83700.3125, Valid Loss: 93767.265625\n",
      "Epoch 435, Train Loss: 83699.5234375, Valid Loss: 93766.640625\n",
      "Epoch 436, Train Loss: 83698.75, Valid Loss: 93766.0390625\n",
      "Epoch 437, Train Loss: 83697.9921875, Valid Loss: 93765.453125\n",
      "Epoch 438, Train Loss: 83697.2265625, Valid Loss: 93764.890625\n",
      "Epoch 439, Train Loss: 83696.484375, Valid Loss: 93764.34375\n",
      "Epoch 440, Train Loss: 83695.7421875, Valid Loss: 93763.828125\n",
      "Epoch 441, Train Loss: 83695.0078125, Valid Loss: 93763.34375\n",
      "Epoch 442, Train Loss: 83694.28125, Valid Loss: 93762.875\n",
      "Epoch 443, Train Loss: 83693.5625, Valid Loss: 93762.4296875\n",
      "Epoch 444, Train Loss: 83692.828125, Valid Loss: 93762.0078125\n",
      "Epoch 445, Train Loss: 83692.1171875, Valid Loss: 93761.59375\n",
      "Epoch 446, Train Loss: 83691.4140625, Valid Loss: 93761.2109375\n",
      "Epoch 447, Train Loss: 83690.6953125, Valid Loss: 93760.84375\n",
      "Epoch 448, Train Loss: 83690.0078125, Valid Loss: 93760.484375\n",
      "Epoch 449, Train Loss: 83689.296875, Valid Loss: 93760.140625\n",
      "Epoch 450, Train Loss: 83688.6015625, Valid Loss: 93759.828125\n",
      "Epoch 451, Train Loss: 83687.90625, Valid Loss: 93759.5\n",
      "Epoch 452, Train Loss: 83687.21875, Valid Loss: 93759.1953125\n",
      "Epoch 453, Train Loss: 83686.515625, Valid Loss: 93758.8984375\n",
      "Epoch 454, Train Loss: 83685.8359375, Valid Loss: 93758.6015625\n",
      "Epoch 455, Train Loss: 83685.140625, Valid Loss: 93758.3046875\n",
      "Epoch 456, Train Loss: 83684.46875, Valid Loss: 93758.03125\n",
      "Epoch 457, Train Loss: 83683.796875, Valid Loss: 93757.765625\n",
      "Epoch 458, Train Loss: 83683.109375, Valid Loss: 93757.4921875\n",
      "Epoch 459, Train Loss: 83682.421875, Valid Loss: 93757.2109375\n",
      "Epoch 460, Train Loss: 83681.7421875, Valid Loss: 93756.9765625\n",
      "Epoch 461, Train Loss: 83681.078125, Valid Loss: 93756.7109375\n",
      "Epoch 462, Train Loss: 83680.4140625, Valid Loss: 93756.4609375\n",
      "Epoch 463, Train Loss: 83679.7578125, Valid Loss: 93756.203125\n",
      "Epoch 464, Train Loss: 83679.0859375, Valid Loss: 93755.9375\n",
      "Epoch 465, Train Loss: 83678.4296875, Valid Loss: 93755.703125\n",
      "Epoch 466, Train Loss: 83677.7734375, Valid Loss: 93755.46875\n",
      "Epoch 467, Train Loss: 83677.125, Valid Loss: 93755.21875\n",
      "Epoch 468, Train Loss: 83676.46875, Valid Loss: 93754.96875\n",
      "Epoch 469, Train Loss: 83675.8046875, Valid Loss: 93754.7265625\n",
      "Epoch 470, Train Loss: 83675.1640625, Valid Loss: 93754.4921875\n",
      "Epoch 471, Train Loss: 83674.5234375, Valid Loss: 93754.2421875\n",
      "Epoch 472, Train Loss: 83673.875, Valid Loss: 93754.015625\n",
      "Epoch 473, Train Loss: 83673.25, Valid Loss: 93753.7734375\n",
      "Epoch 474, Train Loss: 83672.6015625, Valid Loss: 93753.5390625\n",
      "Epoch 475, Train Loss: 83671.9765625, Valid Loss: 93753.3046875\n",
      "Epoch 476, Train Loss: 83671.34375, Valid Loss: 93753.0625\n",
      "Epoch 477, Train Loss: 83670.71875, Valid Loss: 93752.8125\n",
      "Epoch 478, Train Loss: 83670.0859375, Valid Loss: 93752.5859375\n",
      "Epoch 479, Train Loss: 83669.4609375, Valid Loss: 93752.359375\n",
      "Epoch 480, Train Loss: 83668.8515625, Valid Loss: 93752.1171875\n",
      "Epoch 481, Train Loss: 83668.234375, Valid Loss: 93751.890625\n",
      "Epoch 482, Train Loss: 83667.6171875, Valid Loss: 93751.6484375\n",
      "Epoch 483, Train Loss: 83667.0, Valid Loss: 93751.40625\n",
      "Epoch 484, Train Loss: 83666.375, Valid Loss: 93751.1640625\n",
      "Epoch 485, Train Loss: 83665.765625, Valid Loss: 93750.921875\n",
      "Epoch 486, Train Loss: 83665.1640625, Valid Loss: 93750.6875\n",
      "Epoch 487, Train Loss: 83664.5546875, Valid Loss: 93750.4609375\n",
      "Epoch 488, Train Loss: 83663.953125, Valid Loss: 93750.2265625\n",
      "Epoch 489, Train Loss: 83663.3515625, Valid Loss: 93749.984375\n",
      "Epoch 490, Train Loss: 83662.75, Valid Loss: 93749.75\n",
      "Epoch 491, Train Loss: 83662.1484375, Valid Loss: 93749.5234375\n",
      "Epoch 492, Train Loss: 83661.5625, Valid Loss: 93749.296875\n",
      "Epoch 493, Train Loss: 83660.96875, Valid Loss: 93749.0625\n",
      "Epoch 494, Train Loss: 83660.359375, Valid Loss: 93748.8203125\n",
      "Epoch 495, Train Loss: 83659.765625, Valid Loss: 93748.5859375\n",
      "Epoch 496, Train Loss: 83659.1796875, Valid Loss: 93748.3515625\n",
      "Epoch 497, Train Loss: 83658.59375, Valid Loss: 93748.1171875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18080\\336630549.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m#define MSE as loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m#define scaler for target normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m#reshape inputs to add sequence length dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18080\\336630549.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, train_set, valid_set, maxEpoch)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0my_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0my_valid_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxEpoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18080\\336630549.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, xtrain, ytrain)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1059\u001b[0m               output_gradients))\n\u001b[0;32m   1060\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;31m# The parens ensure that if grad is IndexedSlices, it'll get multiplied by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m     \u001b[1;31m# Tensor (not a number like 2.0) which causes it to convert to Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m     \u001b[0mx_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m   if (isinstance(grad, ops.Tensor) and\n\u001b[0;32m   1617\u001b[0m       _ShapesFullySpecifiedAndEqual(x, y, grad)):\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1480\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"math.subtract\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtract\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_binary_elementwise_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  12926\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12927\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12928\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12929\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12930\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  12931\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12932\u001b[0m       return sub_eager_fallback(\n\u001b[0;32m  12933\u001b[0m           x, y, name=name, ctx=_ctx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, model_path, train_mode=True, input_dim=10, lstm_size=256, batch_size=32, e_learning_rate=1e-5):\n",
    "        super(Model, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.train_mode = train_mode\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm_size = lstm_size\n",
    "        self.batch_size = batch_size\n",
    "        self.e_learning_rate = e_learning_rate\n",
    "\n",
    "        #define LSTM layer\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(units=self.lstm_size, return_sequences=True)\n",
    "\n",
    "        #define output layer\n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, activation=None)\n",
    "\n",
    "        #define optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.e_learning_rate)\n",
    "\n",
    "        #define MSE as loss function\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        #define scaler for target normalization\n",
    "        self.target_scaler = MinMaxScaler(feature_range=(0, 500))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #reshape inputs to add sequence length dimension\n",
    "        inputs = tf.expand_dims(inputs, axis=1)\n",
    "        \n",
    "        #input shape: (batch_size, sequence_length, input_dim)\n",
    "        x = self.lstm_layer(inputs)\n",
    "        #output shape: (batch_size, sequence_length, lstm_size)\n",
    "        output = self.output_layer(x)\n",
    "        #output shape: (batch_size, sequence_length, 2) - 2 for outputs\n",
    "        return output\n",
    "\n",
    "    def train_step(self, xtrain, ytrain):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(xtrain, training=True)\n",
    "            loss = self.loss_fn(ytrain, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_set, valid_set, maxEpoch=10):\n",
    "        x_train, y_train = train_set\n",
    "        x_valid, y_valid = valid_set\n",
    "        \n",
    "        #normalize target values\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_valid_scaled = self.target_scaler.transform(y_valid.reshape(-1, 1))\n",
    "\n",
    "        for epoch in range(maxEpoch):\n",
    "            train_loss = self.train_step(x_train, y_train_scaled)\n",
    "            valid_loss = self.loss_fn(y_valid_scaled, self(x_valid, training=False))\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csvfile1 = pd.read_csv('C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/testdata.csv')\n",
    "\n",
    "    #convert the df to numpy array and cast the data to float\n",
    "    results = csvfile1.to_numpy(dtype='float')\n",
    "\n",
    "    #define input indices and output indices\n",
    "    input_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    output_indices = [1, 2] \n",
    "\n",
    "    #split data into train and valid sets\n",
    "    train_size = int(len(results) * 0.9)\n",
    "    train_features = results[:train_size, input_indices]\n",
    "    train_targets = results[:train_size, output_indices]\n",
    "    valid_features = results[train_size:, input_indices]\n",
    "    valid_targets = results[train_size:, output_indices]\n",
    "\n",
    "    #create train and valid sets with input features and targets\n",
    "    train_set = (train_features, train_targets)\n",
    "    valid_set = (valid_features, valid_targets)\n",
    "\n",
    "    #initialize and train the model\n",
    "    mymodel = Model(model_path=\"saved_model\")\n",
    "    mymodel.train(train_set, valid_set, maxEpoch=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
