{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I tried a simple transformer architecture, didnt prove very worthwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Reshape, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.layers import Dropout,  TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import HeNormal\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_concatenated_csv = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/test_data_concat/sorted_concatenated_data.csv\"\n",
    "data = pd.read_csv(sorted_concatenated_csv)\n",
    "data.drop(columns=['zCOM'], inplace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "smallest_values = data[cytokine_columns].min()\n",
    "largest_values = data[cytokine_columns].max()\n",
    "\n",
    "print(\"Smallest values for each cytokine:\")\n",
    "print(smallest_values)\n",
    "print(\"\\nLargest values for each cytokine:\")\n",
    "print(largest_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negative_with_zero(data):\n",
    "    num_negative_values = (data < 0).sum().sum()\n",
    "    data[data < 0] = 0\n",
    "\n",
    "    return num_negative_values\n",
    "\n",
    "cytokine_columns = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "for col in cytokine_columns:\n",
    "    num_negatives = replace_negative_with_zero(data[col])\n",
    "    print(f\"Number of negative values replaced with 0 in '{col}': {num_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "unique_time = data['time'].unique()\n",
    "\n",
    "arrays = {}\n",
    "\n",
    "for time in unique_time:\n",
    "\n",
    "    data_time = data[data['time'] == time]\n",
    "    \n",
    "    array = np.zeros((50, 50, len(cytokines)))\n",
    "    \n",
    "    x = data_time['xCOM'].astype(int)\n",
    "    y = data_time['yCOM'].astype(int)\n",
    " \n",
    "    concentrations = data_time[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "    \n",
    "    array[x, y, :] = concentrations\n",
    "    \n",
    "    arrays[time] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "output_values = []\n",
    "\n",
    "arrays_list = [arrays[key] for key in sorted(arrays.keys())]\n",
    "\n",
    "arrays_np = np.array(arrays_list)\n",
    "\n",
    "for i in range(len(arrays_np) - sequence_length):\n",
    "    input_seq = arrays_np[i:i+sequence_length]  \n",
    "    output_val = arrays_np[i+sequence_length] \n",
    "    \n",
    "    input_sequences.append(input_seq)\n",
    "    output_values.append(output_val)\n",
    "\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_values = np.array(output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics, lr scheduler\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return 1 - SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def average_relative_rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_pred - y_true) / K.clip(K.abs(y_true), K.epsilon(), None))))\n",
    "\n",
    "#def average_relative_error(y_true, y_pred):\n",
    "    return K.mean(K.abs((y_pred - y_true) / K.clip(K.abs(y_true), K.epsilon(), None)))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    abs_diff = K.abs(y_true - y_pred)\n",
    "    threshold = 0.2 * y_true\n",
    "    accurate_predictions = K.less_equal(abs_diff, threshold)\n",
    "    accuracy = K.mean(accurate_predictions)\n",
    "    return accuracy\n",
    "\n",
    "#def explained_variance(y_true, y_pred):\n",
    "    return 1 - K.var(y_true - y_pred) / K.var(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-1\n",
    "    if epoch < 500:\n",
    "        return 1e-2\n",
    "    else:\n",
    "        return 1e-3\n",
    "\n",
    "train_size = int(0.7 * len(input_sequences))\n",
    "val_size = int(0.1 * len(input_sequences))\n",
    "\n",
    "X_train, y_train = input_sequences[:train_size], output_values[:train_size]\n",
    "X_val, y_val = input_sequences[train_size:train_size+val_size], output_values[train_size:train_size+val_size]\n",
    "X_test, y_test = input_sequences[train_size+val_size:], output_values[train_size+val_size:]\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-2]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(input_shape, num_heads, ff_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(8, (3, 3), padding='same', activation='relu'))(inputs)\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(x)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(16, (3, 3), padding='same', activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(x)\n",
    "    \n",
    "    x = layers.Reshape((input_shape[0], -1))(x)\n",
    "    \n",
    "    embed_dim = x.shape[-1]\n",
    "    x = PositionalEncoding(input_shape[0], embed_dim)(x)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(50 * 50 * 6)(x)\n",
    "    outputs = layers.Reshape((50, 50, 6))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (sequence_length, 50, 50, 6)\n",
    "num_heads = 2  \n",
    "ff_dim = 16    \n",
    "model = build_transformer_model(input_shape, num_heads, ff_dim)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[r_squared, 'mape', average_relative_rmse, 'msle', 'mae'])\n",
    "\n",
    "batch_size = 32\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=150, verbose=1, restore_best_weights=True)\n",
    "\n",
    "initial_lr = 1e-1\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=batch_size, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "print(f'Test Metrics: {test_metrics}')\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
